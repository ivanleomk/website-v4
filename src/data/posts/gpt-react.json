{
  "title": "GPT-React",
  "date": "2023-12-20T00:00:00.000Z",
  "description": "Using RAG to generate UI Components",
  "categories": [
    "RAG",
    "UI Generation"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n## Introduction\n\n> The full code for this is avaliable [here](https://github.com/ivanleomk/GPT-React) for reference.\n\nA while ago, I saw a demo video of Vercel's V0 and was blown away by what it could produce. It could take in user prompts, feedback and iteratively generate new and improved UI code using the popular `@shadcn/ui` library.\n\nThis was soon followed by the [open-v0](https://github.com/raidendotai/openv0/) project by [raidendotai](https://twitter.com/n_raidenai). Since I didn't have access to v0 via vercel, i figured I would clone the project and try to figure out how it worked.\n\nOne eventful friday evening later, I ended up putting together a small prototype which uses context-aware RAG and pydantic to generate valid NextJS Code based on a user prompt which you can see below.\n\n> The Gif renders pretty slowly for some reason so if you want to see the original clip, you can check it out [here](https://twitter.com/ivanleomk/status/1715423194973351955)\n\n![](./images/succesful_render.gif)\n\n### Overview\n\nLet's break down how it works under the hood. On a high level, whenerver a user submits a prompt we do the following\n\n1. First, we extract out a list of substasks which might be relevant to solving this problem. We also pass in a list of components and a short description for each of them into the prompt for added context.\n\n2. We then take these subtasks and embed them, performing a lookup on code chunks in our vector db. This vector db contains a embedding map of `task` ( which was generated by gpt 4 ) to a `code chunk` from the `@shadcn/ui` library.\n\n3. We then perform a deduplication on our code chunk extracted so that the model only sees each example once.\n\n4. We also pass in a set of Rules. These are a set of conditions which the model has to conform to.\n\n5. We then get the model to generate the code therefafter.\n\nNote that the entire codebase used `gpt-4` as a default. I didn't have the time to do fine-tuning.\n\n## Data Preparation\n\nAs anyone working with LLMs will tell you, it's most important to have good quality data. In this case, that's very true.\n\nIn my case, I chose to process the data as such\n\n1. First, extract out all of the code chunks in the `@shadcn/ui` library. I used a `dump.json` file which was in the `open-v0` repository to do this.\n\n2. Next, for each code chunk, I got GPT-4 to generate a task that might have led to this code chunk being written.\n\nLet's see a quick example.\n\n```js\nimport { Input } from \"@/components/ui/input\";\n\nexport function InputDemo() {\n  return <Input type=\"email\" placeholder=\"Email\" />;\n}\n```\n\nWhat are some tasks that could have been assigned to a developer who submits a code chunk like this? In this case, GPT-4 came up with\n\n- Create a new component named `InputDemo` that uses the `Input` component from the @shadcn/ui library\n- Set the type of the `Input` component to `email`\n- Set the placeholder of the `Input` component to `Email`\n- Ensure the `InputDemo` component is exported as the default export\n\nSo we perform this task generation for each and every code example that's given in the `@shadcn/ui` library. We ideally want more potential tasks so that for each code chunk, we have more potential options that we can check against when a user makes a query.\n\n### Generating the Tasks\n\nThe original `dump.json` file doesn't store code chunks nicely and has a lot more metadata. So we need to first massage the data.\n\nThis is done by using the `extract_examples_from_data` function in the source code\n\n```py\ndef extract_examples_from_data(doc_data):\n    return [i[\"code\"] for i in doc_data[\"docs\"][\"examples\"]]\n```\n\nOnce we've extracted the source code out, we now have a collection of code chunks. Now let's think a bit about the kind of data structure we expect back. This is where pydantic shines.\n\n```py\nclass Task(BaseModel):\n    \"\"\"\n    This is a class which represents a potential task that could have resulted in the code snippet provided\n\n    eg. I want a button that generates a toast when it's clicked\n    eg. I want a login form which allows users to key in their email and validates that it belongs to the facebook.com domain.\n    \"\"\"\n    task: str = Field(description=\"This is a task which might have resulted in the component\")\n```\n\nit's useful here to point out that the more examples you provide and the more descriptive your class definition is, the better your eventual outputs are going to be. But since we want multiple tasks instead of just one from the code chunk, we can take advantage of the `MultiTask` functionality provided in the `instructor` library.\n\n> Don't forget to run `instructor.patch()` before you run your functions so you get the nice functionality it provides\n\nThis allows us to query GPT-4 for multiple instances of the `Task` object by creating a new pydantic class. In our case, we'll call it `MultiTaskType` so that we don't get a naming conflict.\n\n```py\nMultiTaskType = instructor.MultiTask(Task)\n```\n\nWe can then use this in our code as follows\n\n```py\ncompletion = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=0.3,\n        stream=False,\n        max_retries=2,\n        functions=[MultiTaskType.openai_schema],\n        function_call={\"name\": MultiTaskType.openai_schema[\"name\"]},\n        messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"As an experienced programmer using the NextJS Framework with the @shadcn/ui and tailwindcss library, you are tasked with brainstorming some tasks that could have resulted in the following code chunk being produced.\"\n        },\n        {\n            \"role\":\"assistant\",\n            \"content\":\"Examples of such tasks could be adding a toast component to display temporary messages, using a specific variant avaliable in the @shadcn/ui library or configuring a component that toggles between two display states\"\n        },\n        {\n        \"role\":\"assistant\",\n        \"content\": \"Tasks should be as diverse as possible while staying relevant. Generate at most 4 Tasks.\"\n            },\n        {\n            \"role\": \"user\",\n            \"content\": f\"{chunk}\",\n        },\n        ],\n        max_tokens=1000,\n        )\nres = MultiTaskType.from_response(completion)\nreturn [i.task for i in list(res)[0][1]]\n```\n\nNotice here that we get automatic retries with the new `max_retries` parameter in the `openai` library thanks to patching openai. While running this command, since we're firing out a lot of different api calls to gpt-4, we might also get rate-limited.\n\n> At the time of this article, GPT-4 has a rate-limit of approximately `200` requests per minute. Therefore, in running this script, it's very likely that you might be rate-limited for a large dataset. PLEASE BUILD RATE-LIMITING AND CHECKPOINTS INTO YOUR CODE.\n\nTherefore, I found it useful to run this function async with the following logic\n\n```py\nasync def generate_task_given_chunk(chunk:str,retries=3)->List[str]:\n    for _ in range(retries):\n        try:\n          // Execute code completion code here\n        except Exception as e:\n          print(\"----Encountered an exception\")\n          print(e)\n          await asyncio.sleep(60)\n```\n\nWe can then map over all the examples in a single async call.\n\n```py\ncode_examples:List[str] = extract_examples_from_data(data[mapping[key]])\nloop = asyncio.get_event_loop()\ntasks = [generate_task_given_chunk(code_example) for code_example in code_examples]\nresults = loop.run_until_complete(asyncio.gather(*tasks))\n```\n\nWe then save the results at each step by using\n\n```py\nfor code_example, potential_tasks in zip(code_examples, results):\n    if key not in queries:\n        queries[key] = {}\n    queries[key][code_example] = potential_tasks\n\nwith open(data_dir,\"w+\") as f:\n    json.dump(queries, f)\n```\n\nRunning it on the entire code chunk examples took me about 10-20 minutes so just let it run and get a cup of coffee.\n\n### Chromadb\n\nWhile there are a lot of potential options out there, I wanted a simple to deploy vector db that I could run locally. After some research, I ended up looking at chromadb which provides a python integration and persistent data.\n\n```bash\npip install chromadb\n```\n\nWe can then initialise a persistent Chromadb instance by running\n\n```py\nchroma_client = chromadb.PersistentClient(path= #path here)\n```\n\nSo once we've generated the code tasks and saved it in a giant json file, we can now generate some embeddings for easy lookup. In my case, I'm storing my code in a collection called `task_to_chunk`. I delete the collection on each run of my generate script so that I end up with a fresh embedding database.\n\n```py\ntry:\n  collection = chroma_client.get_collection(collection_name)\n  chroma_client.delete_collection(collection_name)\n  print(f\"Deleted {collection_name}.\")\nexcept Exception as e:\n  print(f\"Collection {collection_name} does not exist...creating now\")\n# We generate the task list, and we have the code. The next step is to then embed each of the individual tasks into a chromadb database\ncollection:chromadb.Collection = chroma_client.get_or_create_collection(collection_name)\n```\n\nWe can then embed the entire list of tasks using the default settings in chromadb\n\n```py\n# Then we embed the individual queries\nfor component in queries.keys():\n    for chunk in queries[component].keys():\n        for task in queries[component][chunk]:\n            id = uuid.uuid4()\n            collection.add(\n                documents=[task],\n                metadatas=[{\n                    \"chunk\":chunk,\n                    \"component\":component,\n                }],\n                ids=[id.__str__()]\n            )\n```\n\nNote here that we embed the task and then store the chunk and component as metadata for easy lookup. We also generate a unique uuid for each task we add into the database.\n\n## Server\n\n### Scaffolding\n\nWe can now generate a small python server using [FastAPI](https://fastapi.tiangolo.com). We can initialise it by creating a simple `main.py` script as seen eblow\n\n```py\nfrom typing import Union\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI\nimport chromadb\nimport dotenv\nimport openai\nimport os\n\ndotenv.load_dotenv()\nopenai.api_key = os.environ[\"OPENAI_KEY\"]\n\napp = FastAPI()\nchroma_client = chromadb.PersistentClient(path=\"./chromadb\")\ncollection = chroma_client.get_collection(\"task_to_chunk\")\n\nclass UserGenerationRequest(BaseModel):\n    prompt:str\n\n@app.post(\"/\")\ndef read_root(Prompt:UserGenerationRequest):\n  return \"OK\"\n```\n\nIn our example above, we\n\n- Loaded an Open AI API Key in the system env\n- Started a chroma instance that we can query against\n- Got a reference to our chromadb collection\n\n### Generating Sub Tasks\n\nNow, that we can take in a user prompt with our endpoint, let's now generate a bunch of sub tasks\n\nSimilar to our earlier example, we'll start by defining our pydantic model\n\n```py\nclass SubTask(BaseModel):\n  \"\"\"\n  This is a class representing a sub task that must be completed in order for the user's request to be fulfilled.\n\n  Eg. I want to have a login form that users can use to login with their email and password. If it's a succesful login, then we should display a small toast stating Congratulations! You've succesfully logged in.\n\n  We can decompose our tasks into sub tasks such as\n  - Create a input form that takes in an email\n  - Create a primary button that has an onclick event\n  - display a toast using the useToast hook\n\n  and so on.\n  \"\"\"\n  task:str = Field(description=\"This is an instance of a sub-task which is relevant to the user's designated task\")\n```\n\nWe can then use the `MultiTask` function from the `instructor` library to get a variety of different tasks using the function below.\n\n```py\ndef generate_sub_tasks(query):\n  completion = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    temperature=0.3,\n    stream=False,\n    functions=[MultiTaskObjects.openai_schema],\n    function_call={\"name\": MultiTaskObjects.openai_schema[\"name\"]},\n    messages=[\n        {\n          \"role\": \"system\",\n          \"content\": f\"You are a senior software engineer who is very familiar with NextJS and the @shadcn/ui libary. You are about to be given a task by a user to create a new NextJS component.\"\n        },\n        {\n          \"role\":\"assistant\",\n          \"content\": \"Before you start on your task, let's think step by step to come up with some sub-tasks that must be completed to achieve the task you are about to be given.\"\n        },\n        {\n          \"role\":\"assistant\",\n          \"content\":f\"Here are some of the components avaliable for use\\n{COMPONENTS}\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"{query}\",\n        },\n    ],\n    max_tokens=1000,\n  )\n  queries = MultiTaskObjects.from_response(completion)\n\n  # General Syntax to get the individual tasks out from the MultiTaskObjects object\n  return [i.task for i in list(queries)[0][1]]\n```\n\nNote here that we're also passing in a list of all the components that our llm has access to. This looks like\n\n```py\nCOMPONENTS = \"\"\"\n-Typography:Styles for headings, paragraphs, lists...etc\n-Accordion:A vertically stacked set of interactive headings that each reveal a section of content.\n-Alert:Displays a callout for user attention.\n-Alert Dialog:A modal dialog that interrupts the user with important content and expects a response.\n-Aspect Ratio:Displays content wi\n# other components go here\n\"\"\"\n```\n\nI found that this helped the LLM to make more accurate decisions when it came to sub tasks since it was able to identify precisely what components it needed. Once we've extracted out the sub tasks, we can then proceed to get the relevant code chunks by computing embeddings.\n\n### Getting Embeddings\n\nI found that chromadb had a slightly awkward way of returning embeddings. Ideally I want it in a giant object which contains just the relevant code chunks but it returns it as a map of\n\n```py\n{\n  ids: [id1,id2]\n  metadatas: [metadata1,metadata2],\n  embeddings: [embedding1, embedding2]\n}\n```\n\nSo, I wrote this little function to extract out the top 3 most relevant code chunks for our example.\n\n````py\nelevant_results = collection.query(\n    query_texts=[user_prompt],\n    n_results=3\n)\n\nctx = []\nuniq = set()\nfor i in range(len(relevant_results[\"metadatas\"])):\n    for code_sample,sample_query in zip(relevant_results[\"metadatas\"][i],relevant_results[\"documents\"][i]):\n        if sample_query not in uniq:\n            ctx.append(f\"Eg.{sample_query}\\n```{code_sample}```\\n\")\n            uniq.add(sample_query)\n\nctx_string = \"\".join(ctx)\n````\n\n> Note here that I'm using the sample_query instead of the code chunk as the unique key for deduplication. This is intentional. I thought that it would be useful to provide a few different ways that the same code chunk could be used.\n\nThis then produces a long context string that looks like this\n\n> Eg. How to code an input box\n>\n> ```js\n> // code chunk goes here\n> ```\n\nWhich we compile and feed in as context.\n\n### Prompting our Model\n\nI found that by using a vanilla prompt, GPT-4 tended to return either\n\n1. Invalid Code - it would hallucinate and create imports from other libraries such as `@chakra-ui`\n2. It would return a short remark before giving me the actual code (Eg. Sure, I can give you the code ... (react code goes here ))\n\nIn order to fix this, I implemented two solutions\n\n1. Rules - these are pieces of information that the model needs to follow. I thought it would be more useful to add it at the end of the prompt because models tend to add additional weight to information placed at the start and end of their prompts\n\n2. A Pydantic Model - This would force it to return code\n\nRules were not overly complex, I ended up using the following after some experimentation. These are useful because libraries tend to have library-specific implementation details and having rules ensures that your model respects them\n\n> Here are some rules that you must follow when generating react code\n>\n> 1. Always add a title and description to a toast\n>\n> ```js\n>    onClick={() => {\n>    toast({\n>       title: // title goes here,\n>     description: // description,\n>     })\n> }}\n> ```\n>\n> 2. Make sure to only use imports that follow the following pattern\n>\n> - 'React'\n> - '@/components/ui/componentName`\n> - 'next/'\n>\n> 3.  No other libraries are allowed to be used\n\nIn terms of the pydantic model, I didn't use anything overtly complex.\n\n```py\nclass CodeResult(BaseModel):\n  \"\"\"\n  This is a class representing the generated code from a user's query. This should only include valid React Code that uses the @shadcn/ui library. Please make to conform to the examples shown\n  \"\"\"\n  code:str\n```\n\nI found that the above code definition was good enough to achieve consistent react code generated throughout all my individual tests. I then utilised gpt-4 to generated the code\n\n```py\ndef generate_code(ctx_string,user_prompt):\n  gen_code: CodeResult = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    response_model=CodeResult,\n    max_retries=2,\n    messages=[\n        {\n          \"role\": \"system\",\n          \"content\": f\"You are a NextJS expert programmer. You are about to be given a task by a user to fulfil an objective using only components and methods found in the examples below.\"\n        },\n        {\n            \"role\":\"assistant\",\n            \"content\":f\"Here are some relevant examples that you should refer to. Only use information from the example. Do not invent or make anything up.\\n {ctx_string}\"\n        },\n        {\n          \"role\":\"assistant\",\n          \"content\":f\"Please adhere to the following rules when generating components\\n {RULES}\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"{user_prompt}\",\n        },\n    ]\n  )\n  return gen_code.code\n```\n\n## Frontend\n\n### Rendering the Generated Code\n\nThe Frontend proved to be a little bit difficult because nextjs would not allow me to inject in the react code generated. This makes sense since normally, we write react code, which is then bundled into a source map and javascript files before it's rendered in the browser.\n\nSo, as a workaround, I realised that I could use a client side dynamic import to render a component. On the backend, we can support this by adding the following snippet into our route.\n\n```py\nwith open('../src/generated/component.tsx', 'w') as f:\n    f.write(generated_code)\n```\n\nWe can then import in this component on the frontend by defining a dynamicly imported component in nextjs\n\n```js\nimport dynamic from \"next/dynamic\";\n\nconst DynamicHeader = dynamic(() => import(\"../generated/component\"), {\n  loading: () => <p>Loading...</p>,\n  ssr: false, // add this line to disable server-side rendering\n});\n```\n\nOnce this was done, we could just import it like any other component and use it. I wrote up a quick and dirty client componet using server actions that would achieve this result.\n\n```js\n\"use client\";\nimport { Button } from \"@/components/ui/button\";\nimport { Input } from \"@/components/ui/input\";\nimport { Label } from \"@/components/ui/label\";\nimport { Textarea } from \"@/components/ui/textarea\";\nimport { useToast } from \"@/components/ui/use-toast\";\nimport { ClearComponent, SubmitPrompt } from \"@/lib/prompt\";\nimport dynamic from \"next/dynamic\";\nimport React, { useState, useTransition } from \"react\";\nimport { ClipLoader } from \"react-spinners\";\nimport { Tabs, TabsContent, TabsList, TabsTrigger } from \"@/components/ui/tabs\";\n\nconst DynamicHeader = dynamic(() => import(\"../generated/component\"), {\n  loading: () => <p>Loading...</p>,\n  ssr: false, // add this line to disable server-side rendering\n});\n\nconst UserInput = () => {\n  const [isPending, startTransition] = useTransition();\n  const [userInput, setuserInput] = useState(\"\");\n  const [generatedCode, setGeneratedCode] = useState(\"\");\n\n  return (\n    <div className=\"max-w-xl mx-auto\">\n      <h1>GPT-4 Powered React Components</h1>\n\n      <form\n        className=\"my-4 \"\n        onSubmit={(e) => {\n          e.preventDefault();\n          startTransition(() => {\n            SubmitPrompt(userInput).then((res) => {\n              setGeneratedCode(res[\"code\"]);\n            });\n          });\n        }}\n      >\n        <Label>Prompt</Label>\n        <Textarea\n          value={userInput}\n          onChange={(e) => setuserInput(e.target.value)}\n          placeholder=\"I want to create a login form that...\"\n        />\n        <div className=\"flex items-center justify-end mt-6 space-x-4\">\n          <Button type=\"submit\">Submit</Button>\n        </div>\n      </form>\n\n      {isPending ? (\n        <>\n          {\" \"}\n          <ClipLoader speedMultiplier={0.4} size={30} /> <span>Generating Component...</span>\n        </>\n      ) : generatedCode.length === 0 ? (\n        <p className=\"text-center\">No Component Generated yet</p>\n      ) : (\n        <Tabs defaultValue=\"code\">\n          <TabsList className=\"grid w-full grid-cols-2\">\n            <TabsTrigger value=\"code\">Code</TabsTrigger>\n            <TabsTrigger value=\"component\">Component</TabsTrigger>\n          </TabsList>\n          <TabsContent value=\"code\">\n            <code className=\"relative rounded px-[0.3rem] py-[0.2rem] font-mono text-sm\">\n              {generatedCode.split(\"\\n\").map((item) => (\n                <div className=\"py-1 px-4 bg-muted\">{item}</div>\n              ))}\n            </code>\n          </TabsContent>\n          <TabsContent value=\"component\">\n            <div className=\"mt-6 border px-6 py-6\">\n              <DynamicHeader />\n            </div>\n          </TabsContent>\n        </Tabs>\n      )}\n    </div>\n  );\n};\n\nexport default UserInput;\n```\n\n## Takeaways\n\nThis was a very fun project to implement as a quick weekend hack and again, it is not production level code at all, not even close.\n\n### Main Problems\n\nI think there are a few main things that plague this project\n\n1. A lack of bundler validation\n2. Insufficent Data\n3. Hacky way of rendering components\n\nOne of the best ways to validate the react code is to have access to a javascript bundler which can deterministically tell you if a given chunk of code is valid or not.\n\nHowever, I'm not too sure how to do that at all using python. As a result, sometimes we would end up with code that was almost correct but was missing a few imports. This would have been easy to catch if we had access to a bundler\n\nAnother thing that my model suffers from is that of a simple dataset. The embedding search and lookup is as good as the quality of examples that we feed into it. However, since the tasks are being manually generated by GPT-4 and we have no easy way of evolving the given examples into more complex ones without spending more time on it, this limits the ability of our model to generate complex examples.\n\nI personally found that when we gave it more complex examples, it started making references to objects it didn't know how to mock. Code like the following would be generated\n\n```js\n<span>{user.email}</span> // user is not mocked at all, throwing an error\n```\n\nLastly, rendering components using a dynamic import is very hacky. I personally don't think it's the best way but I was unable to think of an easy way to bundle the generated code and injecting it as html to my endpoint.\n\n### Future Improvements\n\nIf I had more time to work on this, I'd probably work on the following things\n\n1. Fine Tuning : A large chunk of these tasks can probably rely on a well tuned gpt3.5 model. Examples could be generating small and specific task files (Eg. helper functions, simple input and label forms)\n\n2. Using a LLM Validator : I recently learnt that `instructor` also provides a `llm_validator` class that you can validate outputs against. That might have been a better place to use the rules validation that we defined.\n\n3. Using multiple agents : I think that this project is best served if we could look towards more specialised agents for specific tasks (Eg. One for design, one for planning, another for writing tests etc). Lindy does something similiar with calendar and emails.\n\n4. Dataset Improvements : It would be interesting to use something like evol-instruct to come up with either more complex prompts in our database or more complex code examples that show how to build increasingly difficult schemas. I strongly believe this would have improved the quality of the generatde output.\n",
  "slug": "gpt-react"
}