{
  "title": "How does Instructor work?",
  "date": "2024-09-08T00:00:00.000Z",
  "description": "How your request goes from chat completion to validated Pydantic model",
  "categories": [
    "instructor"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\nFor Python developers working with large language models (LLMs), `instructor` has become a popular tool for structured data extraction. While its capabilities may seem complex, the underlying mechanism is surprisingly straightforward. In this article, we'll walk through a high level overview of how the library works and how we support the OpenAI Client.\n\nWe'll start by looking at\n\n1. Why should you care about Structured Extraction?\n2. What is the high level flow\n3. How does a request go from Pydantic Model to Validated Function Call?\n\nBy the end of this article, you'll have a good understand of how `instructor` helps you get validated outputs from your LLM calls and a better understanding of how you might be able to contribute to the library yourself.\n\n### Why should you care?\n\nFor developers integrating AI into production systems, structured outputs are crucial. Here's why:\n\n1. **Validation Reliability** : As AI-driven data extraction becomes more complex, manually crafting validation logic grows increasingly error-prone. Structured outputs provide a robust framework for ensuring data integrity, especially for nested or intricate data structures.\n2. **System Integration** : Incorporating AI-generated content into existing infrastructure demands predictable, well-defined output formats. Structured outputs act as a bridge, allowing seamless integration of AI capabilities with established systems and workflows.\n\nBy leveraging tools that enforce structured outputs, developers can harness the power of AI while maintaining control over data quality and system reliability. This approach not only streamlines development but also enhances the robustness of AI-driven applications.\n\n> In short, structured outputs transform unvalidated LLM calls into validated functions with type signatures that behave exactly as a normal python function, albeit with some level of probablistic behaviour.\n\n## High Level Flow\n\nLet's look at the `Getting Started` example from the [docs](https://python.useinstructor.com/) and see how it works. In this article, we'll only be looking at the synchronous implementation of the `chat.completions.create` function.\n\n```python\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n)\n\nprint(user_info.name)\n#> John Doe\nprint(user_info.age)\n#> 30\n```\n\nA few things are happening here\n\n1. We define our desired output structure using `Pydantic`\n2. We wrap our client in a `from_openai` function that returns a client with the same interface but patched with our new functionality.\n3. We then make a call like we normally do to `OpenAI`'s API, but with the exception of the a new `response_model` parameter.\n4. Magically we get our output?\n\nThat to me was an incredible experience compared to something like Langchain which abstracted a significant amount of inner workings away that made customisation difficult. Now that we've seen how it works on an API level, let's look at what the library does under the hood.\n\n## Parsing Responses and Handling Errors\n\nLet's try to answer a few questions here:\n\n1. What does the `from_openai` function do?\n2. How does the Pydantic `response_model` keyword argument get used?\n3. What happens to the response from the LLM and how is it validated when we use `response_model`?\n\n### The `from_openai` function\n\nWe can see the code for the `from_openai` function [here](https://github.com/jxnl/instructor/blob/ea9c1dc5fd40795088bda7b18bc2b3312b32c087/instructor/client.py#L390) where it takes in two main arguments - `client` and `Mode`. These `Mode` enums are how we switch between different modes of interaction with the OpenAI client itself.\n\n```python\nclass Mode(enum.Enum):\n    \"\"\"The mode to use for patching the client\"\"\"\n\n    FUNCTIONS = \"function_call\"\n    PARALLEL_TOOLS = \"parallel_tool_call\"\n    TOOLS = \"tool_call\"\n    MISTRAL_TOOLS = \"mistral_tools\"\n    JSON = \"json_mode\"\n    MD_JSON = \"markdown_json_mode\"\n    JSON_SCHEMA = \"json_schema_mode\"\n    ANTHROPIC_TOOLS = \"anthropic_tools\"\n    ANTHROPIC_JSON = \"anthropic_json\"\n    COHERE_TOOLS = \"cohere_tools\"\n    VERTEXAI_TOOLS = \"vertexai_tools\"\n    VERTEXAI_JSON = \"vertexai_json\"\n    GEMINI_JSON = \"gemini_json\"\n    GEMINI_TOOLS = \"gemini_tools\"\n    COHERE_JSON_SCHEMA = \"json_object\"\n    TOOLS_STRICT = \"tools_strict\"\n```\n\nFor `OpenAI`, we have the following tools\n\n1. `FUNCTIONS` - This was the previous method of calling OpenAI functions that's been deprecated\n2. `TOOLS_STRICT` - This is the current Tool Calling mode that uses Structured Outputs\n3. `TOOLS` - This is how we call OpenAI tools and is the default mode for the OpenAI client\n4. `JSON` - This is when we manually prompt the LLM to return JSON and then parse it using a JSON loader.\n\n```python\ndef from_openai(\n    client: openai.OpenAI | openai.AsyncOpenAI,\n    mode: instructor.Mode = instructor.Mode.TOOLS,\n    **kwargs: Any,\n) -> Instructor | AsyncInstructor:\n    # Other Validation Log Here\n\n    if isinstance(client, openai.OpenAI):\n        return Instructor(\n            client=client,\n            create=instructor.patch(create=client.chat.completions.create, mode=mode),\n            mode=mode,\n            provider=provider,\n            **kwargs,\n        )\n\n    if isinstance(client, openai.AsyncOpenAI):\n        return AsyncInstructor(\n            client=client,\n            create=instructor.patch(create=client.chat.completions.create, mode=mode),\n            mode=mode,\n            provider=provider,\n            **kwargs,\n        )\n```\n\nWe can see here that when we use the `from_openai` function, we get a new `Instructor` that has been patched with our desired mode. What's this [`.patch` function](https://github.com/jxnl/instructor/blob/ea9c1dc5fd40795088bda7b18bc2b3312b32c087/instructor/patch.py#L80) doing? In short, it's really helping us to create a new function that wraps the original `client.chat.completions.create` function that exists on the `Instructor` class that we've now obtained from the `from_openai` function.\n\n```python\ndef patch(\n    client: Union[OpenAI, AsyncOpenAI] = None,\n    create: Callable[T_ParamSpec, T_Retval] = None,\n    mode: Mode = Mode.TOOLS,\n) -> Union[OpenAI, AsyncOpenAI]:\n    # ... Validation Logic\n\n    @wraps(func)\n    def new_create_sync(\n        response_model: type[T_Model] = None,\n        validation_context: dict = None,\n        max_retries: int = 1,\n        strict: bool = True,\n        *args: T_ParamSpec.args,\n        **kwargs: T_ParamSpec.kwargs,\n    ) -> T_Model:\n        response_model, new_kwargs = handle_response_model(\n            response_model=response_model, mode=mode, **kwargs\n        )\n        response = retry_sync(\n            func=func,\n            response_model=response_model,\n            validation_context=validation_context,\n            max_retries=max_retries,\n            args=args,\n            strict=strict,\n            kwargs=new_kwargs,\n            mode=mode,\n        )\n        return response\n\n    new_create = new_create_async if func_is_async else new_create_sync\n\n    if client is not None:\n        client.chat.completions.create = new_create\n        return client\n    else:\n        return new_create\n```\n\nThe key insight here is that the magic happens with\n\n- `handle_response_model` - This is where we do a lot of the heavy lifting. We use the `response_model` to convert your Pydantic class into a OpenAI Schema compatible format.\n- `retry_sync` - This is where we handle the retry logic. We use the `max_retries` to retry the function call if it fails.\n\n### How does the Pydantic `response_model` keyword argument get used?\n\nLet's first look at the code for the `handle_response_model` function [here](https://github.com/jxnl/instructor/blob/ea9c1dc5fd40795088bda7b18bc2b3312b32c087/instructor/process_response.py#L176)\n\n```python\ndef handle_response_model(\n    response_model: type[T] | None, mode: Mode = Mode.TOOLS, **kwargs: Any\n) -> tuple[type[T], dict[str, Any]]:\n    \"\"\"Prepare the response model type hint, and returns the response_model\n    along with the new modified kwargs needed to be able to use the response_model\n    parameter with the patch function.\n\n\n    Args:\n        response_model (T): The response model to use for parsing the response\n        mode (Mode, optional): The openai completion mode. Defaults to Mode.TOOLS.\n\n    Raises:\n        NotImplementedError: When using stream=True with a non-iterable response_model\n        ValueError: When using an invalid patch mode\n\n    Returns:\n        Union[Type[OpenAISchema], dict]: The response model to use for parsing the response\n    \"\"\"\n    new_kwargs = kwargs.copy()\n\n    # Other Provider Logic\n    if not issubclass(response_model, OpenAISchema):\n        response_model = openai_schema(response_model)  # type: ignore\n\n    # Other Logic\n    elif mode in {Mode.TOOLS, Mode.MISTRAL_TOOLS}:\n            new_kwargs[\"tools\"] = [\n                {\n                    \"type\": \"function\",\n                    \"function\": response_model.openai_schema,\n                }\n            ]\n            if mode == Mode.MISTRAL_TOOLS:\n                new_kwargs[\"tool_choice\"] = \"any\"\n            else:\n                new_kwargs[\"tool_choice\"] = {\n                    \"type\": \"function\",\n                    \"function\": {\"name\": response_model.openai_schema[\"name\"]},\n                }\n\n\n    # Other Logic\n\n    return response_model, new_kwargs\n```\n\nWe can see here that we've converted the `response_model` into a format that's compatible with the OpenAI API. This is where the `openai_schema` function is called. This function is responsible for converting your Pydantic class into a format that's compatible with the OpenAI API, code can be found [here](https://github.com/jxnl/instructor/blob/ea9c1dc5fd40795088bda7b18bc2b3312b32c087/instructor/function_calls.py#L348)\n\n```python\nclass OpenAISchema(BaseModel):\n    # Ignore classproperty, since Pydantic doesn't understand it like it would a normal property.\n    model_config = ConfigDict(ignored_types=(classproperty,))\n\n    @classproperty\n    def openai_schema(cls) -> dict[str, Any]:\n        \"\"\"\n        Return the schema in the format of OpenAI's schema as jsonschema\n\n        Note:\n            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n        Returns:\n            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n        \"\"\"\n        schema = cls.model_json_schema()\n        docstring = parse(cls.__doc__ or \"\")\n        parameters = {\n            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n        }\n        for param in docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                if \"description\" not in parameters[\"properties\"][name]:\n                    parameters[\"properties\"][name][\"description\"] = description\n\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n\n        if \"description\" not in schema:\n            if docstring.short_description:\n                schema[\"description\"] = docstring.short_description\n            else:\n                schema[\"description\"] = (\n                    f\"Correctly extracted `{cls.__name__}` with all \"\n                    f\"the required parameters with correct types\"\n                )\n\n        return {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": parameters,\n        }\n\ndef openai_schema(cls: type[BaseModel]) -> OpenAISchema:\n    if not issubclass(cls, BaseModel):\n        raise TypeError(\"Class must be a subclass of pydantic.BaseModel\")\n\n    schema = wraps(cls, updated=())(\n        create_model(\n            cls.__name__ if hasattr(cls, \"__name__\") else str(cls),\n            __base__=(cls, OpenAISchema),\n        )\n    )\n    return cast(OpenAISchema, schema)\n```\n\nWith this function, we're able to take our original Pydantic class and convert it to a function call that looks something like this.\n\n```python\n{\n    \"name\": \"UserInfo\",\n    \"description\": \"A user info object\",\n    \"parameters\": {\n        \"name\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the user\"\n        },\n        \"age\": {\n            \"type\": \"int\",\n            \"description\": \"The age of the user\"\n        }\n    }\n}\n```\n\nWe then customise the specific kwargs that we'll then be passing into the OpenAI API to call a function that matches the exact Pydantic class we've defined.\n\n```python\nnew_kwargs[\"tools\"] = [\n    {\n        \"type\": \"function\",\n        \"function\": response_model.openai_schema,\n    }\n]\nif mode == Mode.MISTRAL_TOOLS:\n    new_kwargs[\"tool_choice\"] = \"any\"\nelse:\n    new_kwargs[\"tool_choice\"] = {\n        \"type\": \"function\",\n        \"function\": {\"name\": response_model.openai_schema[\"name\"]},\n    }\n```\n\n### How does the response from the LLM get validated?\n\nNow that we've seen how the `response_model` is used, let's look at how the response from the LLM is validated in the `retry_sync` function [here](https://github.com/jxnl/instructor/blob/ea9c1dc5fd40795088bda7b18bc2b3312b32c087/instructor/retry.py#L150)\n\nIt really is a\n\n```python\nfor i in max_retries:\n    try:\n        call_openai_with_new_arguments(**kwargs)\n    except validation fails as e:\n        update the kwargs with the new errors ( keep appending to the messages the generated content + validation errors )\n```\n\nYou'll see this from the code snippet below\n\n```python\ndef retry_sync(\n    func: Callable[T_ParamSpec, T_Retval],\n    response_model: type[T_Model],\n    validation_context: dict,\n    args,\n    kwargs,\n    max_retries: int | Retrying = 1,\n    strict: bool | None = None,\n    mode: Mode = Mode.TOOLS,\n) -> T_Model:\n\n    # Compute some stuff\n\n    try:\n        response = None\n        for attempt in max_retries:\n            with attempt:\n                try:\n                    response = func(*args, **kwargs)\n                    stream = kwargs.get(\"stream\", False)\n\n                    return process_response(\n                        response,\n                        response_model=response_model,\n                        stream=stream,\n                        validation_context=validation_context,\n                        strict=strict,\n                        mode=mode,\n                    )\n                except (ValidationError, JSONDecodeError) as e:\n                    if <condition unrelated to TOOL calling with OpenAI>:\n                        raise e\n                    else:\n                        kwargs[\"messages\"].extend(reask_messages(response, mode, e))\n```\n\nResk messages themselves aren't anything special, for tool calling, we're literally just appending the `response` from the LLM and the `validation_context` to the messages and calling the LLM again as you can see [here](https://github.com/jxnl/instructor/blob/main/instructor/retry.py#L34)\n\n```python\ndef reask_messages(response: ChatCompletion, mode: Mode, exception: Exception):\n    # other Logic\n    if mode in {Mode.TOOLS, Mode.TOOLS_STRICT}:\n        for tool_call in response.choices[0].message.tool_calls:\n            yield {\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call.id,\n                \"name\": tool_call.function.name,\n                \"content\": f\"Validation Error found:\\n{exception}\\nRecall the function correctly, fix the errors\",\n            }\n```\n\nThis updates the messages with the validated errors that we've been passing into the OpenAI API and then we call the LLM again. Eventually either we get the validated response that we care about or we hit the max retry limit and raise an error.\n\n## Why you probably shouldn't roll your own\n\nI hope this article has shed some light on the inner workings of Instructor and how it's able to provide a seamless experience for structured outputs. If anything, I hope it helps you understand and think about how you might be able to contribute to the library yourself in the future.\n\nWhile it might be tempting to implement your own solution, there are several challenges to consider:\n\n1. Constantly tracking updates to different LLM providers can be time-consuming and difficult to maintain.\n2. Implementing your own streaming support, partial responses, and iterable handling is technically challenging and prone to errors.\n\nInstead of reinventing the wheel, using a validated library like Instructor allows you to focus on what truly matters - building your LLM application. By leveraging a robust, well-maintained solution, you can save time, reduce errors, and stay up-to-date with the latest developments in the rapidly evolving field of LLMs.\n",
  "slug": "how-does-instructor-work"
}