{
  "title": "Writing scripts that scale",
  "date": "2024-04-19T00:00:00.000Z",
  "description": "A few actionable tips to writing better machine learning scripts",
  "categories": [
    "LLMs"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\nWriting good scripts for machine learning is an art. I struggled with writing them for a long time because of how different it was to my experience working with full-stack frameworks such as React or FastAPI.\n\nThere were four main issues that I struggled with\n\n1. My job has a high probability of failing without any reason\n2. My data might not fit into memory for no reason\n3. Running a single job takes days or more\n4. Optimizing hyper-parameters is genuinely difficult\n\nThis means that when we write these scripts, there are a different set of considerations that need to be kept in mind. I find it useful to keep these few things in mind when writing my training scripts.\n\n1. Write your pipeline first\n2. Build in regular checkpoints\n3. Use generators\n4. Implement Logging\n\n## Write your pipeline first\n\nBefore starting on the big training job, it's important to make sure that your entire pipeline is working the way you want it to be. A good way to detect errors in your pipeline is to first work with a smaller model and dataset before going for a large YOLO run.\n\nI typically work with a training dataset of 1000-2000 values when I'm writing these scripts. My goal is to ideally have something that can be ran in < 60s at most so that I can check for any quick implementation issues. This has caught many different bugs in my codebase because we can iterate and experiment quickly. ( **Note** : Depending on your pipeline, this might be even smaller, for some projects I've done just 30-50 values for the initial stage )\n\nHere's an example below using the hugging face `datasets` library where I take a small slice of 20 items that have been filtered using the `.filter` method.\n\n```python\nfrom datasets import load_dataset\n\nselected_repos = set([\"facebook/react\"])\ntotal_fetched_rows = 20\ndataset = (\n    load_dataset(\"bigcode/the-stack-github-issues\", split=\"train\", streaming=True)\n    .filter(lambda row: row[\"repo\"] in selected_repos)\n    .take(total_fetched_rows)\n)\n```\n\nThe goal is really to make sure everything works end to end.\n\n## Using Checkpoints\n\nIt's not uncommon for models to fail midway through a long training run either due to a timeout issue or insufficient memory capacity inside the CUDA GPU that you're using to train\n\n![Solving the “RuntimeError: CUDA Out of memory” error | by Nitin Kishore |  Medium](https://miro.medium.com/v2/resize:fit:1400/1*enMsxkgJ1eb9XvtWju5V8Q.png)\n\nTherefore, you'll need to implement some form of checkpoints so that if training fails, you can just resume it from an earlier version. This has been a lifesaver in many circumstances and many standard libraries should support it out of the box.\n\n## Use Generators/Dataloaders\n\nGenerators allow you to get significant performance improvements because we can load in data on-demand. This is big because many of our datasets will not be able to fit into the memory of a single CPU/GPU.\n\nIntuitively, we can use generators to save time.\n\n```python\ndef get_all_keys(data):\n    return [row[\"key\"] for row in data]\n\n\ndef get_all_keys_with_generator(data):\n    for row in data:\n        yield row[\"key\"]\n\n```\n\nIf we look at the example above, we can immediately start consuming the `row['key']` data in the first row with the generator syntax. In the first example, we needed to wait for the every single row in memory to be processed and the key added to the new list that the list comprehension would create.\n\nWhat's really cool about generators is that we can chain them together. This ensures that we process our data quickly but also that we do so in the specific order that we want. A common problem in RAG is to read in documents, so let's see how we might be able to write a few generators to do the job.\n\n```python\nfrom pathlib import Path\n\n\ndef read_files(path: Path, file_suffix: str):\n    for file in path.iterdir():\n        if file.suffix != file_suffix:\n            continue\n        yield {\"file\": file.name, \"content\": file.read_text()}\n\n\ndef chunk_text(documents):\n    for document in documents:\n        for chunk in document[\"content\"].split(\"\\n\"):\n            yield {\"chunk\": chunk, \"file\": document[\"file\"]}\n\n\ndef batch_items(items, batch_size=20):\n    batch = []\n    for item in items:\n        batch.append(item)\n        if len(batch) == batch_size:\n            yield batch\n            batch = []\n\n    if batch:\n        yield batch\n\n\nfiles = read_files(Path.cwd(), \"md\")\nchunks = chunk_text(files)\nbatches = batch_items(chunks)\n```\n\nImagine if we had an incredibly large number of documents, we'd be waiting for every file to be read in and every chunk that we'd ever process to be loaded into memory before we even started batching items. With generators, we're able to both reason about the order that our data is consumed AND get the huge cost savings in execution time.\n\n## Implement Logging\n\nLogging doesn't have to be complex. In fact, for most purposes, a simple `.csv` or a `.json` file will work well for most experiments since you'll be able to throw it into GPT-4 to do some simple data exploration once you've obtained the results.\n\nThere are two main things that I think are important\n\n1. use an append-only file for your logs so you don't override any previous results in your logs\n2. make sure to list the raw events so that you can do further data processing. Try your best to avoid any magic numbers\n\n```python\n# Save the results to a markdown file, this is useful for viewing the results in a human readable format\nwith open(\"./output.md\", \"a+\") as f:\n    for row in data:\n        f.write(\n            json.dumps(\n                {\n                    \"batch_size\": 3,\n                    # List out parameters of job here\n                }\n            )\n            + \"\\n\"\n        )\n\n```\n\nWe can see an example here of magic numbers in a json format. It's immediately obvious that I have no idea what `12348`, `8` or `24` represent and the longer you spend away from your code, the less likely you will anyway.\n\n```\n{\n    12348: {\"accuracy\": 24, \"recall\": 123},\n    8: {\"accuracy\": 24, \"recall\": 123},\n    24: {\"accuracy\": 24, \"recall\": 123},\n}\n```\n\nInstead log data with all the parameters named so that it's easier to work through it later like aforementioned. Examples of this can be seen below where we not only encode in all of the parameters explicitly, we also utilise a `.jsonl` format where each entry is separated by a `\\n` character.\n\n```\n{{\"accuracy\": 24, \"recall\": 123, \"sample_size\": 12348}\n{\"accuracy\": 24, \"recall\": 123, \"sample_size\": 8}\n{\"accuracy\": 24, \"recall\": 123, \"sample_size\": 24}\n```\n\nThis json can then be read in down the line with a function which is similar to what we see below.\n\n```py\ndef read_chunks_from_jsonl(path: str):\n    chunks = []\n    with open(path, \"r\") as file:\n        for line in file:\n            chunk = CommentChunk(**json.loads(line))\n            chunks.append(chunk)\n    return chunks\n```\n\nIf you're looking to do more advanced logging, weights and biases is a useful addition to your toolbox that you can consider.\n\n## Use Pydantic\n\nThis is a small bonus tip but try to use pydantic where possible. It's a great tool in your scripts to buiild in simple validation checks.\n\n## Conclusion\n\nI hope you found this small article useful. I'm new to the machine learning space and changing the way I write python scripts has helped me get much better results with my scripts. With these small changes, I'm confident that you'll see huge improvements in your scripts and their results.\n",
  "slug": "write-better-scripts"
}