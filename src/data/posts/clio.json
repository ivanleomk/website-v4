{
  "title": "Understanding User Conversations",
  "date": "2025-01-04T00:00:00.000Z",
  "description": "How do we understand Conversations at Scale while preserving user privacy?",
  "categories": [
    "Clustering",
    "LLMs"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n# Using Language Models to make sense of Chat Data without compromising user privacy\n\n> If you're interested in the code for this article, you can find it [here](https://github.com/ivanleomk/chatterbox) where I've implemented a simplified version of CLIO without the PII classifier and most of the original prompts ( to some degree ).\n\nAnalysing chat data at scale is a challenging task for 3 main reasons\n\n1. **Privacy** - Users don't want their data to be shared with others and we need to respect that. This makes it challenging to do analysis on user data that's specific\n2. **Explainability** - Unsupervised clustering methods are sometimes difficult to interpret because we don't have a good way to understand what the clusters mean.\n3. **Scale** - We need to be able to process large amounts of data efficiently.\n\nAn ideal solution allows us to understand broad general trends and patterns in user behaviour while preserving user privacy. In this article, we'll explore an approach that addresses this challenge - Claude Language Insights and Observability ( CLIO ) which was recently discussed in a research paper released by Anthropic.\n\nWe'll do so in 3 steps\n\n1. We'll start by understanding on a high level how CLIO works\n2. We'll then implement a simplified version of CLIO in Python\n3. We'll then discuss some of the clusters that we generated and some of the limitations of such an approach\n\nLet's walk through these concepts in detail.\n\n## CLIO\n\n### High Level Overview\n\nYou can read about CLIO in [Anthropic's blog post here](https://www.anthropic.com/research/clio). Personally I just asked Claude to explain it to me and it did a good job.\n\nOn a high level, it uses two main techniques to understand user conversations at scale\n\n1. **Clustering** - Anthropic uses a K-Means clustering algorithm to group similar conversations/clusters together using their embeddings. The exact hyper-parameter of `k` wasn't disclosed in their paper but they mention it's a function of the dataset they use.\n2. **LLMs Labels** - Given a set of clusters or conversation summaries, they use a combination of `claude-3.5-haiku` and `claude-3-opus` to generate new potential clusters that combine these child clusters and recursively apply this process until we have a number of clusters that we want.\n\nWe can see this from the diagram below where we have initial private user conversations that get increasingly summarised and clustered so that the broad general trends are preserved while omitting the specific user details.\n\n![](./images/clio.webp)\n\nThis in turn creates overall patterns that can be used to understand user behaviour as seen below\n\n![](./images/use_cases.webp)\n\n### What's interesting about this\n\nI think there are three interesting things about this approach that really stood out to me from the original paper\n\n#### Synthetic Data\n\nCLIO was validated by using a largely synthetic dataset that was generated by Claude. This comprised approximately ~20k conversations that spanned across 15 different languages and 18 high level topics.\n\n![](./images/clio_synthetic_data.png)\n\nThe final clusters were then manually validated by Anthropic's team to ensure that they were meaningful. This is interesting because they then used it as a way to systematically probe specific weaknesses that Claude has for specific models.\n\n#### PII Classifier\n\nAnthropic also used a PII classifier to ensure that conversations were anonymised. Their definition of PII was interesting and I'll quote it here\n\n![](./images/clio_pii.png)\n\nThey bootstrapped this classifier by using a small dataset of initial user messages that they had manualy labelled as non PII and then synthetically generated more data by deliberately adding PII to the data. This allowed them to see how their `haiku` performed on this specifc task - showing that with the summarisation step, we were able to remove PII in 98.5% of the data.\n\n![](./images/clio_pii_results.png)\n\n#### Explainability\n\nBy using a language model to generate their clusters and descriptions and a language model for a classifier, Anthropic was able to generate a significantly larger amount of data on why specific clusters were generated or why a conversation was labelled as exposing PII.\n\nThis in turn helped to develop more fine grained classifiers for their PII classifier as well as tune their prompts for the language models as seen below.\n\n![](./images/clio_safety.png)\n\n## Implementing CLIO\n\nNow that we understand on a high level how CLIO works, let's implement a simplified version of it in Python. We'll be using `gemini-1.5-flash` since it's cheaper to use and I have higher rate-limits on it. I've put together a [Github Repository with the code here](https://github.com/ivanleomk/chatterbox) where you can find the code for this article.\n\nIt also contains a FastHTML application that you can use to generate and visualise your own clusters.\n\n![](https://r2-workers.ivanleomk9297.workers.dev/clio.gif)\n\nWe'll do so in 3 steps.\n\n1. **Summarisation** : We'll first start by generating a condensed summary of each conversation using `gemini-1.5-flash`.\n2. **Clustering** : We'll then show how we can use a language model to cluster these summaries and create an explainable cluster\n3. **Hierachal Clustering** : Finally, we'll show how our recursive clustering approach works to generate a hierachal clustering of the data\n\nBecause our prompts will get a bit more complex, we'll use `instructor` to help us format our prompts nicely with jinja templating support and the ability to obtain structured outputs.\n\n### Summarisation\n\nCLIO works by generating a condensed summary of each conversation. For each, we pass in the conversation and extract out two main components\n\n| Component        | Description                                               | Examples                                                                                                                                                                                                                                                           |\n| ---------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| User Request     | A description of what the user is asking for              | - The user's overall request for the assistant is to help implementing a React component to display a paginated list of users from a database<br>- The user's overall request for the assistant is to debug a memory leak in their Python data processing pipeline |\n| Task Description | A description of the task generated by the language model | - The task is to help build a frontend component with React and implement database integration<br>- The task is to debug performance issues and optimize memory usage in Python code                                                                               |\n\nWe can do so with the function defined below\n\n```python\ndef summarise_conversation(client, conversation) -> dict:\n    \"\"\"\n    Given a user conversation, returns a summary of the user request and task description.\n    \"\"\"\n    resp = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"Summarize the conversation by providing:\n                1. A user request starting with \"The user's overall request for the assistant is to\"\n                2. A task description starting with \"The task is to\"\n\n                Keep each to 1-2 sentences and max 20 words. Include relevant technical details like programming languages and frameworks.\n                Exclude any PII (names, locations, emails, companies, etc).\n\n                Example User Request:\n                - The user's overall request for the assistant is to help implementing a React component to display a paginated list of users from a database.\n\n                Example Task Description:\n                - The task is to help build a frontend component with React and implement database integration\n\n                Here is the conversation:\n                {% for message in messages %}\n                <message>{{message.role}}: {{message.content}}</message>\n                {% endfor %}\n                \"\"\"\n            }\n        ],\n        context={\"messages\": conversation.messages},\n        response_model=ConversationSummary,\n    )\n\n    return {\n        \"chat_id\": conversation.chat_id,\n        \"task_description\": resp.task_description,\n        \"user_request\": resp.user_request,\n        \"turns\": len(conversation.messages),\n    }\n```\n\nNotice here how we're also extracting out the `turns` in this conversation here. Depending on your specific use-case, you may want to extract out other bits of information like the language, user_id, organisation etc so that you can implement deduplication or other forms of analysis.\n\nThis allows us to take a conversation and extract out the user request and task description. For instance, given the exchange below, we can extract out the user request and task description as follows\nHere's an example conversation:\n\n| Role      | Message                                                                                                                                                       |\n| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Human     | what's the weather in Tokyo?                                                                                                                                  |\n| Assistant | The weather in Tokyo is currently 20 degrees Celsius with clear skies.                                                                                        |\n| Human     | Thanks, what should I wear out in the city today? I'm headed into Ginza for a quick shopping trip with Jake my best friend before i go into the OpenAI office |\n| Assistant | It's a bit chilly, so I recommend wearing a light jacket and jeans.                                                                                           |\n\nThis would be processed into:\n\n| Field            | Content                                                                                         |\n| ---------------- | ----------------------------------------------------------------------------------------------- |\n| User Request     | The user's overall request for the assistant is to get weather and clothing recommendations     |\n| Task Description | The task is to provide weather information and suggest appropriate clothing based on conditions |\n\nWe can see that with the summarisation step, we've removed the PII ( The exact company he works at, the name of his friend etc) and we've also removed the location ( Tokyo, Ginza etc). Instead, we've extracted out the rough user request and task description that the user is trying to achieve - in this case get weather and clothing recommendations.\n\n### Clustering\n\nOnce we've obtained a few summaries, we can then just embed them and use a simple K-Means clustering algorithm to group them together.\n\nI used the `text-embedding-3-small` model to embed the summaries and then used `sklearn` to perform the clustering.\n\n```python\ndef cluster_items(items: list[dict], items_per_cluster: int) -> list[dict]:\n    n_clusters = math.ceil(len(items) // items_per_cluster)\n    embeddings = [item[\"embedding\"] for item in items]\n    X = np.array(embeddings)\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    cluster_labels = kmeans.fit_predict(X)\n\n    group_to_clusters = {}\n    for i, item in enumerate(items):\n        item_copy = item.copy()\n        cluster_id = int(cluster_labels[i])\n        del item_copy[\"embedding\"]\n        if cluster_id not in group_to_clusters:\n            group_to_clusters[cluster_id] = []\n\n        group_to_clusters[cluster_id].append(item_copy)\n\n    return group_to_clusters\n```\n\nOnce we've done so, we obtain a list of conversation summaries that we can then use to generate a cluster. What's cool about CLIO's approach is that it uses positive and contrastive examples to generate this cluster.\n\n```python\nasync def generate_base_cluster(client, sem, user_requests: list[dict], negative_examples: list[dict]):\n    async with sem:\n        cluster = await client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                Summarize related statements into a short description and name. Create a concise two-sentence summary\n                in past tense that captures the essence and distinguishes from other groups.\n\n                Generate a specific name under 10 words in imperative form (e.g. \"Help debug Python code\").\n                Be descriptive and distinguish from contrastive examples.\n                \"\"\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\"\"\n                Here are the relevant statements:\n                <positive_examples>\n                {% for item in user_requests %}\n                    <positive_example>{{ item['user_request'] }} : {{ item['task_description'] }}</positive_example>\n                {% endfor %}\n                </positive_examples>\n\n                For context, here are statements from other groups:\n\n                <contrastive_examples>\n                {% for item in negative_examples %}\n                <contrastive_example>{{ item['user_request'] }} : {{ item['task_description'] }}</contrastive_example>\n                {% endfor %}\n                </contrastive_examples>\n\n                Analyze both sets of statements to create an accurate summary and name.\n                \"\"\"\n                }\n            ],\n            response_model=ClusterSummary,\n            context={\n                \"user_requests\": user_requests,\n                \"negative_examples\": negative_examples\n            }\n        )\n\n        return Cluster(\n            name=cluster.name,\n            description=cluster.summary,\n            chat_ids=[item[\"chat_id\"] for item in user_requests],\n            parent_id=None\n        )\n```\n\nFor instance, given the following user chat summaries\n\n| User Request                                                                                                         | Task Description                                                                            |\n| -------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n| The user's overall request for the assistant is to help build a React data table with sorting functionality          | The task is to implement a reusable React component for displaying and sorting tabular data |\n| The user's overall request for the assistant is to implement a paginated user list component in React                | The task is to create a React component that handles pagination and displays user records   |\n| The user's overall request for the assistant is to create a React component for displaying filtered database records | The task is to develop a React component that can filter and display data from a database   |\n\nand the following contrastive examples\n\n| User Request                                                                                    | Task Description                                                                         |\n| ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n| The user's overall request for the assistant is to debug a Python memory leak                   | The task is to identify and fix memory leaks in a Python application                     |\n| The user's overall request for the assistant is to get restaurant recommendations in their area | The task is to provide personalized dining suggestions based on location and preferences |\n| The user's overall request for the assistant is to analyze a CSV file using pandas              | The task is to perform data analysis on a CSV dataset using the pandas library           |\n\nWe could instead generate a cluster with the following name and description\n\n| Name                               | Description                                                                                                                                                                                                                                                                                                        |\n| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| Help implement Frontend Components | The task is to help build reusable React components for displaying and manipulating tabular data, including features like sorting, pagination, and filtering. These components need to integrate with backend data sources and provide a smooth user experience for viewing and interacting with database records. |\n\nThis is a much more descriptive name and description that captures the essence of the user request and task description. At the same time it doesn't include any PII information.\n\n### Hierachal Clustering\n\nCLIO implements Hierachal Clustering in 3 main steps\n\n1. First we generate new candidate clusters by using the existing clusters and their descriptions\n2. Then we label each of our original clusters with our new cluster ( creating a new cluster for each of our original clusters)\n3. We then repeat this process until we've reduced the number of clusters to a number that we want\n\nThis allows us to generate a hierachal clustering of the data.\n\nLet's see this in action\n\n#### Generating Candidate Clusters\n\nLet's start by looking at a few examples of clusters here that I've added below.\n\n!!! cluster \"Categorize and analyze business expenses\"\n\n    The user requested assistance with categorizing and analyzing business expenses, including generating expense categories, identifying overlapping categories, and improving pricing plans. This involved tasks such as creating comprehensive expense code lists, brainstorming department expense usage, and understanding financial reporting concepts like deferred revenue and customer lifetime value (CLV).\n\n!!! cluster \"Generate realistic, ambiguous business transactions and general ledger codes\"\n\n    The user requested assistance generating general ledger codes and ambiguous business transactions, aiming for realistic yet ambiguous classifications spanning multiple categories. This involved tasks such as generating mock transactions, improving transaction naming and classification, and creating challenging examples for annotation and improved accounting software functionality.\n\n!!! cluster \"Improve code using Pydantic and Instructor\"\n\n    The user requested assistance with various tasks involving Pydantic and Instructor, including code improvement, debugging, and model building. These tasks focused on refining code clarity, enhancing functionality, and addressing specific validation or structural issues within the context of these tools.\n\n!!! cluster \"Explain concurrent programming concepts in Rust\"\n\n    The user requested assistance with understanding various aspects of concurrent programming in Rust, including memory ordering, threads, atomic operations, reader-writer locks, interior mutability, and crate management. These requests focused on improving the user's understanding of concurrent programming techniques within the Rust programming language.\n\nWith the following function, we can then generate a list of candidate cluster names. In the case below, we have 4 clusters, and we want to generate at msot 3 candidate clusters.\n\nThis might result in the following candidate cluster names\n\n- Analyze and categorize business financial data\n- Improve and debug code and application\n- Explain Rust and Python code\n\n```python\nasync def generate_candidate_cluster_names(\n    client: instructor.Instructor, sem: Semaphore, clusters: list[Cluster]\n) -> list[str]:\n    resp = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                Create higher-level cluster names based on the provided clusters and descriptions. Generate broader categories that could include multiple clusters while remaining specific and meaningful.\n\n                Review these clusters:\n                <cluster_list>\n                    {% for cluster in clusters %}\n                    <cluster>{{ cluster.name }}: {{ cluster.description }}</cluster>\n                    {% endfor %}\n                </cluster_list>\n\n                Create at most {{ desired_number }} higher-level cluster names. The names should:\n                - Represent broader themes found in multiple clusters\n                - Be specific enough to be meaningful\n                - Be distinct from each other\n                - Use clear, descriptive language\n                - Accurately describe sensitive or harmful topics when present\n\n                Generate fewer than {{ desired_number }} names if that better captures the clusters.\n                \"\"\",\n            }\n        ],\n        response_model=CandidateClusters,\n        context={\n            \"clusters\": clusters,\n            \"desired_number\": math.ceil(\n                (len(clusters) * 3) // 4\n            ),  # This ensure we get at least 3/4 of the clusters ( so we have a 25% reduction or more at each stage)\n        },\n    )\n\n    return resp.candidate_cluster_names\n```\n\n#### Labeling Clusters\n\nNow that we have a list of candidate cluster names, we can then label each of our original clusters with our new cluster. It's important here that we need to **shuffle the cluster names that we present to the language model** so that we don't bias the language model.\n\n```python\nasync def rename_higher_level_cluster(\n    client: instructor.AsyncInstructor,\n    sem: Semaphore,\n    clusters: dict,\n) -> dict:\n    async with sem:\n        resp = await client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                    Create a concise name and description for these related clusters.\n\n                    The name should be:\n                    - Max 10 words\n                    - An imperative sentence\n                    - Specific and descriptive\n                    - Accurately describe sensitive topics\n\n                    The description should be:\n                    - Two sentences in past tense\n                    - Clear and precise\n                    - Specific to this cluster\n\n                    Below are the related cluster names:\n                    <clusters>\n                        {% for cluster in clusters %}\n                            <cluster>{{ cluster.name }}: {{ cluster.description }}</cluster>\n                        {% endfor %}\n                    </clusters>\n                    \"\"\",\n                }\n            ],\n            context={\"clusters\": clusters},\n            response_model=ConsolidatedCluster,\n        )\n\n    return {\n        \"new_cluster\": Cluster(\n            name=resp.cluster_name,\n            description=resp.cluster_description,\n            chat_ids=[chat_id for cluster in clusters for chat_id in cluster.chat_ids],\n            parent_id=None,\n        ),\n        \"original_clusters\": clusters,\n    }\n```\n\nOnce we've done so, we can then repeat this process for all of the clusters that we have. With this, we've generated a list of clusters that are smaller than the lst of clusters we started with. We've also labelled each of our original clusters with our new cluster.\n\nIn our case, this might result in the following clusters and their labels\n\n| Original Cluster Name                                                        | Consolidated Cluster Label                     |\n| ---------------------------------------------------------------------------- | ---------------------------------------------- |\n| Categorize and analyze business expenses                                     | Analyse and categorize business financial data |\n| Generate realistic, ambiguous business transactions and general ledger codes | Analyse and categorize business financial data |\n| Improve code using Pydantic and Instructor                                   | Improve and debug code and application         |\n| Explain concurrent programming concepts in Rust                              | Improve and debug code and application         |\n\nOnce we've done so, we've now reduced our original 4 clusters to 2 clusters. But notice here that we only have a label. That's where a third step comes in.\n\n### Generating New Clusters\n\nNow that we've succesfully clustered our clusters, we have a list of base clusters with a name and a description. We can then use this to generate a new cluster with a more descriptive name and description.\n\n```python\nasync def rename_higher_level_cluster(\n    client: instructor.AsyncInstructor,\n    sem: Semaphore,\n    clusters: dict,\n) -> dict:\n    async with sem:\n        resp = await client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                    You are tasked with summarizing a group of related cluster names into a short, precise, and accurate overall description and name. Your goal is to create a concise summary that captures the essence of these clusters\n\n                    The cluster name should be at most ten words long (perhaps less) and be specific but also reflective of most of the clusters that comprise them. The cluster name should be a sentence in the imperative that captures the user’s request. For example, ‘Brainstorm ideas for a birthday party’ or ‘Help me find a new job are good examples.\n\n                    Do not shy away from describing socially harmful or sensitive topics. For instance, \"Write fantasy sexual roleplay with octopi and monsters\", \"Generate blog spam for gambling websites\", or \"Assist with high school math homework\" would be better than \"Write erotic content\" or \"Help with homework\". Be as descriptive as possible and assume neither good nor bad faith. Do not hesitate to identify and describe socially harmful or sensitive topics specifically; specificity is necessary for monitoring.\n\n                    The cluster description should be a clear, precise, two-sentence description in the past tense. This description should be specific to this cluster.\n\n                    Below are the related cluster names\n                    <clusters>\n                        {% for cluster in clusters %}\n                            <cluster>{{ cluster.name }}: {{ cluster.description }}</cluster>\n                        {% endfor %}\n                    </clusters>\n\n                    Ensure your summary and name accurately represent the clusters and are specific to the clusters.\n                    \"\"\",\n                }\n            ],\n            context={\"clusters\": clusters},\n            response_model=ConsolidatedCluster,\n        )\n\n    return {\n        \"new_cluster\": Cluster(\n            name=resp.cluster_name,\n            description=resp.cluster_description,\n            chat_ids=[chat_id for cluster in clusters for chat_id in cluster.chat_ids],\n            parent_id=None,\n        ),\n        \"original_clusters\": clusters,\n    }\n```\n\nThis in turn gives us a new cluster with a name and description. We can then repeat this process until we've reduced the number of clusters to a number that we want. In our case, this gives us the following clusters\n\n!!! cluster \"Help analyze and categorize business financial data\"\n\n    The user requested assistance with analyzing and categorizing financial data, including expense tracking, transaction classification, and financial metrics analysis. The tasks involved creating expense categories, generating sample transactions, and building tools for financial reporting and analysis.\n\n!!! cluster \"Debug and optimize code across programming languages\"\n\n    The user requested help with debugging, optimizing and improving code quality across multiple programming languages and frameworks. The tasks focused on performance optimization, concurrent programming patterns, and developing robust database-driven applications.\n\nWith this, we've now generated a hierachal clustering of the data! That's basically the high level overview of what CLIO does and how we can use `instructor` to implement it.\n\n## Limitations\n\nIn this article, we've walked through how we can implement CLIO in python. However, there are a few limitations that we need to be aware of.\n\n1. **Unvalidated Clustering Parameters** : Currently the K-Means cluster counts are chosen arbitrarily by taking the total number of conversations and dividing it by the target cluster size. Since the choice of `k` directly impacts how well clusters capture distinct conversational patterns, we need to be more careful here.\n\n2. **Basic PII Removal** : In Anthropic's case, they have a specific PII classifier that they use to remove PII. However, this is a very basic classifier that only looks at the user request and task description. A production system should invest time into building out a more sophisticated PII classifier that can be used to remove PII from the data.\n\n3. **More Concrete Evaluations**: Ultimately, if you're looking to implement CLIO, you'll want to have a more concrete evaluation framework in place. This will allow you to measure the quality of your clusters and ensure that you're getting the most out of your data. Anthropic does this by using a combination of manual validation and synthetic data generation but you should also validate manually that the clusters are meaningful and that the PII is removed.\n   Our implementation of CLIO revealed four main technical limitations that affect its practical usage:\n\n## Conclusion\n\nCLIO offers a promising approach to understanding user conversations at scale while preserving privacy through its combination of summarization, clustering, and hierarchical organization. While our implementation demonstrates the core concepts, a production system would benefit greatly from domain expert involvement to validate cluster quality and ensure the insights are actionable for specific business needs.\n\nUltimately, it's important to reiterate that clustering algorithms are just ways for you to understand your data. They're not absolute truths and you should always be careful to validate your results. The goal isn't perfect clustering but rather gaining meaningful, privacy-preserving insights that can drive product improvements while maintaining user trust.\n",
  "slug": "clio"
}