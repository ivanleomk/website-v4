{
  "title": "Everything I've learnt about writing good Python code",
  "date": "2024-04-17T00:00:00.000Z",
  "description": "Speedrun your way to becoming a good python developer and don't make the same mistakes I did",
  "categories": [
    "Python",
    "Advice"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\nIn the past 6 months, I've 10xed the amount of python code I've written. In this article, I'll show you a few easy actionable tips to write better and more maintainable code. I've been lucky enough to have Jason (@jxnlco on twitter) review a good chunk of my code and I've found that these few things have made a massive difference in my code quality.\n\n1. using the `@classmethod` decorator\n2. learn the stdlib\n3. write simpler functions\n4. being a bit lazier - earn the abstraction\n5. decouple your implementation\n\n## Use the classmethod decorator\n\nYou should be using the `@classmethod` decorator when dealing with complex logic. A good example is that of the [Instructor API schema](https://python.useinstructor.com/api/?h=openai#instructor.function_calls.OpenAISchema) which has clear explicit ways for you to instantiate the different API providers.\n\nLet's compare two separate versions of the API. The first is the API that the library used before their v1.0.0 release and the second is their more recent version\n\n```python\n# Pre-V1.0.0\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n# Post V1\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n```\n\nBecause we're using the classmethod to define explicitly the client that we want to patch, **we get better code readability and improved autocomplete out of the box. this is great for developer productivity**.\n\nIf you ever want to migrate to a separate provider that doesn't support the OpenAI standard, you need to change to a separate classmethod and explicitly make that change in the code. If the two providers have very different behaviour, this helps you to catch subtle bugs that you otherwise might not have caught.\n\nThis is important because the more complex your code's logic is, the more likely for it to have a strange bug. You don't want to be dealing with complex edge cases since you're explicitly declaring the specific clients you're using in your code base.\n\n### Using Classmethods\n\nI recently worked on a script that generated embeddings using different models using SentenceTransformers, OpenAI and Cohere. This was tricky because each of these models need to be used differently, even when initialising them and I finally settled on the code below.\n\n```python\nimport enum\n\n\nclass Provider(enum.Enum):\n    HUGGINGFACE = \"HuggingFace\"\n    OPENAI = \"OpenAI\"\n    COHERE = \"Cohere\"\n\n\nclass EmbeddingModel:\n    def __init__(\n        self,\n        model_name: str,\n        provider: Provider,\n        max_limit: int = 20,\n        expected_iterations: int = float(\"inf\"),\n    ):\n        self.model_name = model_name\n        self.provider = provider\n        self.max_limit = max_limit\n        self.expected_iterations = expected_iterations\n\n    @classmethod\n    def from_hf(cls, model_name: str):\n        return cls(\n            model_name,\n            provider=Provider.HUGGINGFACE,\n            max_limit=float(\"inf\"),\n        )\n\n    @classmethod\n    def from_openai(cls, model_name: str, max_limit=20):\n        return cls(model_name, provider=Provider.OPENAI, max_limit=max_limit)\n\n    @classmethod\n    def from_cohere(cls, model_name: str):\n        return cls(model_name, provider=Provider.COHERE)\n\n```\n\nThere are a few things which make the code above good\n\n1. **Easier To Read**: I can determine which provider I'm using when I instantiate the class - `EmbeddingModel.from_hf` makes it clear that it's the `SentenceTransformers` package that's being used\n\n2. **Lesser Redundancy**: I only need to pass in the values that I need to for each specific model. This makes it easy to add in additional configuration parameters down the line and be confident that it won't mess up existing functionality\n\n## Learn Common Libraries\n\nThis might be overstated but I think everyone should take some time to at least read through the basic functions in commonly used libraries. Some general parallels I've found have been\n\n- Handling Data -> Pandas\n- Retrying/Exception Handling -> Tenacity\n- Caching data -> diskcache\n- Validating Objects -> Pydantic\n- Printing/Formatting things to the console - Rich\n- Working with generators - itertools has a great selection of things like `islice` and automatic batching\n- Writing common counters/dictionary insertion logic etc - use Collections\n- Caching Data/Working with Curried functions? - use functools\n\nIf a commonly used libarary provides some functionality, you should use it. It's rarely going to be beneficial to spend hours writing your own version unless it's for educational purposes. The simple but effective hack I've found has been to use a variant of the following prompt.\n\n```bash\nI want to do <task>. How can I do so with <commonly used library>.\n```\n\nChatGPT has a nasty habit of trying to roll its own implementation of everything. I made this mistake recently as usual when I had to log the results of an experiment I did. ChatGPT suggested I use the `csv` module, manually calculate a set of all of the keys in my data before writing it to a `.csv` file as seen below.\n\n```python\nimport csv\n\ndata = [{\"key1\": 2, \"key4\": 10}, {\"key3\": 3}, {\"key4\": 4}]\n\nkeys = set()\nfor obj in data:\n    keys.update(obj.keys())\n\nwith open(\"output.csv\", \"w\", newline=\"\") as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=keys)\n    writer.writeheader()\n    writer.writerows(data)\n```\n\nAfter spending 30 minutes testing and fixing some bugs with this version, I discovered to my dismay that Pandas had the native `to_csv` classmethod to write to csv and that I could generate a Dataframe from a list of objects as seen below.\n\n```python\nimport pandas as pd\n\ndata = [{\"key1\": 2, \"key4\": 10}, {\"key3\": 3}, {\"key4\": 4}]\ndf = pd.DataFrame(data)\ndf.to_csv(\"output.csv\", index=False)\n```\n\nWhat's beautiful about using a pandas dataframe is that now, you get all this beautiful added functionality like\n\n- Generating it as a markdown table? - just use `df.to_markdown()`\n- Want to get a dictionary with the keys of each row? - just use `df.to_dict()`\n- Want to get a json formatted string? - just use `df.to_json()`\n\nThat's a huge reduction in the potential issues with my code because I'm now using a library method that other people have spent time and effort to write and test. Standard libraries are also well supported across the ecosystem, allowing you to take advantage of other integrations down the line (Eg. LanceDB supporting Pydantic Models )\n\n## Write Simpler Functions\n\nI think that there are three big benefits to writing simpler functions that don't mutate state\n\n1. They're easier to reason about since they're much smaller in size\n2. They're easier to test because we can mock the inputs and assert on the outputs\n3. They're easier to refactor because we can swap out different components easily\n\nI had a pretty complex problem to solve recently with some code - which was to take in a dataset of rows with some text and then embed every sentence inside it. I took some time and wrote an initial draft that looked something like this\n\n```python\ndef get_dataset_batches(data, dataset_mapping: dict[str, int], batch_size=100):\n    \"\"\"\n    In this case, dataset_mapping maps a sentence to a\n    id that we can use to identify it by. This is an\n    empty dictionary by default\n    \"\"\"\n    batch = []\n    for row in data:\n        s1, s2 = data[\"text\"]\n        if s1 not in dataset_mapping:\n            dataset_mapping[s1] = len(dataset_mapping)\n            batch.append(s1)\n            if len(batch) == batch_size:\n                yield batch\n                batch = []\n\n        if s2 not in dataset_mapping:\n            dataset_mapping[s2] = len(dataset_mapping)\n            batch.append(s1)\n        if len(batch) == batch_size:\n            yield batch\n            batch = []\n\n    if batch:\n        yield batch\n```\n\nInstead of doing this, a better method might be to break up our function into the following few smaller functions as seen below.\n\n```python\ndef get_unique_sentences(data):\n    seen = set()\n    for row in data:\n        s1, s2 = data[\"text\"]\n        if s1 not in seen:\n            seen.add(s1)\n            yield s1\n\n        if s2 not in seen:\n            seen.add(s1)\n            yield s2\n\n\ndef get_sentence_to_id_mapping(sentences: List[str]):\n    return {s:i for i, s in enumerate(sentence)}\n\n\ndef generate_sentence_batches(sentence_mapping: dict[str, int], batch_size=100):\n    batch = []\n    for sentence in sentence_mapping:\n        batch.append([sentence, sentence_mapping[sentence]])\n        if len(batch) == batch_size:\n            yield batch\n        batch = []\n\n    if batch:\n        yield batch\n\n```\n\n> I wrote my own batching function here but you should really be using `itertools.batched` if you're running Python 3.12 and above.\n\nWe can then call the function above using a main function like\n\n```python\ndef main(data):\n    sentences = get_unique_sentences(data)\n    s2id = get_sentence_to_id_mapping(sentences)\n    batches = generate_sentence_batches(s2id)\n    return batches\n```\n\nIn the second case we're not squeezing everything into a single function. It's clear exactly what is happening in each function which makes it easy for people to understand your code.\n\nAdditionally, because we don't mutate state in between our functions and instead generate a new value, we are able to mock and test each of these functions individually, allowing for more stable code to be written in the long run.\n\nIt helps to have one main function call a sequence of other functions and have those functions be as flat as possible. This means that ideally between each of these calls, we minimise mutation or usage of some shared variable and only do so when there's an expensive piece of computation involved.\n\n## Earn the Abstraction\n\nI think it's easy to quickly complicate a codebase with premature abstractions without much effort over time - just look at Java. After all, it's a natural reflex as we work towards writing code that is DRY. But that often makes it difficult for you to adapt your code down the line.\n\nFor instance, an easy way to do this initially is to just return a simple dictionary if the returned value is only being used by a single function.\n\n```python\ndef extract_attributes(data):\n    new_data = process_data(data)\n    return {\"key1\": new_data[\"key1\"], \"key2\": new_data[\"key2\"]}\n\n\ndef main():\n    data = pd.read_csv(\"data.csv\")\n    attributes = extract_attributes(data)\n    return attributes\n```\n\nIn this example, there's a limited utility to declaring an entire dataclass because it adds additional overhead and complexity to the function.\n\n```python\n@dataclass\nclass Attributes:\n    key1: List[int]\n    key2: List[int]\n\n\ndef extract_attributes(data):\n    new_data = process_data(data)\n    return Attributes(key1=new_data[\"key1\"], key2=new_data[\"key2\"])\n\n\ndef main():\n    data = pd.read_csv(\"data.csv\")\n    attributes = extract_attributes(data)\n    return attributes\n```\n\n## Decouple your implementation\n\nAnother example I like a lot is scoring values. Say we want to calculate the recall for a list of predictions that we've made where we have a single known label as our ground truth and a list of other labels as our model's predictions. We might implement it down the line as\n\n```python\ndef calculate_recall(labels,predictions):\n    scores = []\n    for label,preds in zip(labels,predictions):\n        calculate_recall(label,preds)\n    return scores\n```\n\nBut what if we'd like to work with other metrics down the line like precision, NDCG or Mean Reciprocal Rank? Wouldn't we then have to declare 4 different functions for this?\n\n```python\ndef calculate_recall_predictions(labels, predictions):\n    scores = []\n    for label, preds in zip(labels, predictions):\n        calculate_recall(label, preds)\n    return scores\n\n\ndef calculate_ndcg_predictions(labels,predictions):\n    scores = []\n    for label, preds in zip(labels, predictions):\n        calculate_ndcg(label, preds)\n    return scores\n```\n\nA better solution instead is to parameterise the scoring function itself. If you look at the different functions we've defined, they all take in a single label. We're also doing the same thing in each function, which is to score the predictions with respect to a specific output.\n\nThis means that we could rewrite this as seen below\n\n```python\ndef score(labels, predictions, score_fn):\n    return [score_fn(label, pred) for label, pred in zip(labels, predictions)]\n```\n\nIn fact, we could even go one step further and just represent all of the metrics that we want as a single dictionary.\n\n```python\nSIZES = [3, 10, 15, 25]\nmetrics = {\"recall\": calculate_recall, \"mrr\": calculate_mrr}\nevals = {}\n\n\ndef predictions_at_k(score_fn, k:int):\n    def wrapper(chunk_id, predictions):\n        return score(chunk_id, predictions[:k])\n\n    return wrapper\n\n\nfor metric, k in itertools.product(metrics.keys(), SIZES):\n    evals[f\"{metric}@{k}\"] = predictions_at_k(score_fn=metrics[metric], k=k)\n```\n\nWe can then take a given label and list of predictions and calculate the result as\n\n```python\ndef score(labels, predictions):\n    return pd.DataFrame(\n        [\n            {label: metric_fn(label, pred) for label, metric_fn in evals.items()}\n            for label, pred in zip(labels, predictions)\n        ]\n    )\n```\n\nThis is an extremely flexible function that we're using which gives us a pandas dataframe out of the box. All we need to do if we want to add an extra metric is to add a new entry to the `metrics` dictionary. If we want to evaluate our results at a new subset of `k` items, we just need to update the `SIZES` array too.\n\n## Conclusion\n\nEveryone needs to write enough bad code to start writing better code. The path to writing better code is paved with a lot of PR reviews and reading better code examples. I've definitely written my share of bad code and I hope that this article helps you to see some interesting ways to write better code.\n",
  "slug": "better-python-code"
}