{
  "title": "What it means to look at your data",
  "date": "2024-12-09T00:00:00.000Z",
  "description": "Learn how to evaluate and improve your LLM systems",
  "categories": [
    "LLMs",
    "Debugging"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\nPeople always talk about looking at your data but what does it actually mean in practice?\n\nIn this post, I'll walk you through a short example. After examining failure patterns, we discovered our query understanding was aggressively filtering out relevant items. We improved the recall of a filtering system that I was working on from 0.86 to 1 by working on prompting the model to be more flexible with its filters.\n\nThere really are two things that make debugging these issues much easier\n\n1. A clear objective metric to optimise for - in this case, I was looking at recall ( whether or not the relevant item was present in the top `k` results )\n2. A easy way to look at the data - I like using `braintrust` but you can use whatever you want.\n\nUltimately debugging these systems is all about asking intelligent questions and systematically hunting for failure modes. By the end of the post, you'll have a better idea of how to think about data debugging as an iterative process.\n\n## Background\n\nIn this specific case, our filtering system takes in a user query and returns a list of products that match the user's query. We can think of it as follows.\n\n```mermaid\ngraph LR\n    %% Styling\n    classDef primary fill:#f8fafc,stroke:#64748b,stroke-width:2px\n    classDef secondary fill:#f8fafc,stroke:#94a3b8,stroke-width:1px\n    classDef highlight fill:#f8fafc,stroke:#64748b,stroke-width:2px\n\n    %% Main Flow\n    A([User Query]) --> B{Extract Filters}:::highlight\n    B --> |Raw Query| C([Search]):::primary\n    B --> |Filters| D{Validate}:::highlight\n    D --> C\n    C --> E{Post-Filter}:::highlight\n    E --> F([Results]):::primary\n\n    %% Filter Types\n    subgraph Filters[\" \"]\n        direction TB\n        G[Category]:::secondary\n        H[Price]:::secondary\n        I[Product Type]:::secondary\n        J[Attributes]:::secondary\n    end\n\n    %% Connect filters\n    E -.-> G\n    E -.-> H\n    E -.-> I\n    E -.-> J\n\n    %% Layout\n    linkStyle default stroke:#94a3b8,stroke-width:1px\n```\n\nWe can represent these filters using a Pydantic schema as seen below where we have category/subcategory specific attributes Eg. sleeve length, fit etc\n\n```python\nclass Attribute(BaseModel):\n    name: str\n    values: list[str]\n\n\nclass QueryFilters(BaseModel):\n    attributes: list[Attribute]\n    material: Optional[list[str]]\n    min_price: Optional[float] = None\n    max_price: Optional[float] = None\n    subcategory: str\n    category: str\n    product_type: list[str]\n    occasions: list[str]\n```\n\nThe goal of the system is to return a list of products that match the user's query. The system is evaluated using the following metrics:\n\n- Recall@k - whether or not the relevant item was present in the top `k` results\n- MRR@k - the mean reciprocal rank of the relevant item in the top `k` results\n\nHere's a simplified hierachy of what our catalog looks like - in actuality we have a much more complex hierarchy with many more categories and subcategories.\n\n```\nCategory: Women\n└── Subcategory: Bottoms\n    └── Product Types:\n        - Jeans\n        - Pants\n        - Shorts\n└── Subcategory: Tops\n    └── Product Types:\n        - Blouses\n        - T-Shirts\n        - Tank Tops\n```\n\n## Initial Implementation: Finding Failure Patterns\n\nOur initial metrics looked promising but had clear room for improvement:\n\n| Metric    | Filtered Results | Semantic Search |\n| --------- | ---------------- | --------------- |\n| recall@3  | 79.00%           | 60.53%          |\n| recall@5  | 79.00%           | 71.05%          |\n| recall@10 | 86.00%           | 84.21%          |\n| recall@15 | 86.00%           | 86.84%          |\n| recall@25 | 86.00%           | 94.74%          |\n\nIf we were to look at the raw numbers, we might be tempted to think that structured extraction is the problem - in other words, perhaps we shouldn't apply the filters at all.\n\nBut that's the wrong way to think about it. We need to understand _why_ we were failing and ultimately using structured metadata filters ensures we serve up reliable and accurate results that conform to what users want.\n\nSo I started looking at specific examples where the relevant item failed to appear in the top `k` results.\n\n```python\nQuery: \"Looking for comfortable, high-rise pants... prefer durable denim...\"\n\nActual Item:\n- Category: Women\n- Subcategory: Bottoms\n- Product Type: Jeans\n- Title: \"High-Waist Blue Jeans\"\n\nGenerated Filters:\ncategory: Women\nsubcategory: Bottoms\nproduct_type: [Pants]  # Would you make this mistake as a human?\n```\n\nThis failure is interesting because a human would never make this mistake. When someone asks for \"pants in denim,\" they're obviously open to jeans. This insight led to our first iteration.\n\n## First Iteration: Think Like a Human\n\nWe broadened category matching to mirror human thinking and added to our structured extraction prompt additional rules as seen below. These would make the filters more relaxed and help to catch cases where the user might be a bit too vague.\n\n- When users mention \"pants\", include both pants and jeans\n- When users ask for \"tops\", include both blouses and tank tops\n\nHere's how it looked in practice:\n\n```python\nQuery: \"Looking for a comfy cotton top for summer...\"\n\nActual Item:\n- Category: Women\n- Subcategory: Tops\n- Product Type: Blouses\n- Title: \"Sleeveless Eyelet Blouse\"\n\nPrevious Filters: [Tank Tops]  # Too literal\nUpdated Filters: [Tank Tops, Blouses]  # More human-like reasoning\n```\n\nThis change improved our metrics:\n\n| Metric    | Updated Results | Previous Results |\n| --------- | --------------- | ---------------- |\n| recall@3  | 81.58%          | 79.00%           |\n| recall@5  | 81.58%          | 79.00%           |\n| recall@10 | 89.47%          | 86.00%           |\n| recall@15 | 89.47%          | 86.00%           |\n| recall@25 | 89.47%          | 86.00%           |\n\nWe've managed to increase recall by 3% - not a huge improvement but it's a start. So we looked a few of the other failing examples to see if we could improve recall further.\n\n## Trigger-Happy Model Filters\n\nOur next set of failures revealed an interesting pattern:\n\n```python\nQuery: \"Looking for women's bottoms... in denim\"\n\nActual Item:\n- Category: Women\n- Subcategory: Bottoms\n- Product Type: Shorts\n- Title: \"Classic Denim Shorts\"\n\nGenerated Filters:\nproduct_type: [Jeans]  # Why assume a specific type?\n```\n\nThis led to a new insight - our model was choosing a specific product type at each step even though users didn't specify it at all. While the resonse model itself allows for multiple product types ( or an empty list to indicate that everything is acceptable ), the structured extraction model was being too strict.\n\nTherefore, we added the following rule to the structured extraction prompt so that it would be more flexible with its filters.\n\n```\nOnly choose a product type if the user explicitly mentions it -\notherwise, stay at the category level\n```\n\nThe results were dramatic:\n\n| Metric    | Final Results | Initial Results |\n| --------- | ------------- | --------------- |\n| recall@3  | 92.11%        | 79.00%          |\n| recall@5  | 92.11%        | 79.00%          |\n| recall@10 | 100.00%       | 86.00%          |\n| recall@15 | 100.00%       | 86.00%          |\n| recall@25 | 100.00%       | 86.00%          |\n\n## Conclusion\n\nWhen looking at your data, it's important to realise that the goal is not to chase a perfect 100% score. Language Models are inherently probablistic systems and we should be looking to understand where they tend to trip up.\n\nBy looking for specific patterns in our evaluation data, we can systematically improve our system and work towards a more robust application. Each iteration should look at specific failure modes and test a hypothesis about why the system is failing.\n\nBy slowly but surely tackling these issues, we can add guardrails or safeguards to our system so that it becomes more reliable and robust over time.\n",
  "slug": "looking-at-your-data"
}