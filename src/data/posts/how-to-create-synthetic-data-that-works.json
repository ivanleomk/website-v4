{
  "title": "How to create synthetic data that works",
  "date": "2024-08-27T00:00:00.000Z",
  "description": "Lessons from generating a few million tokens of synthetic data with gpt-4o-mini",
  "categories": [
    "LLMs",
    "Synthetic Data"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\nSynthetic data can accelerate AI development, but generating high-quality datasets remains challenging. In this article, I'll walk through a few experiments I've done with synthetic data generation and the takeaways I've learnt so that you can do the same.\n\nWe'll do by covering\n\n1. **Limitations of simple generation methods** : Why simple generation methods produce homogeneous data\n2. **Entropy and why it matters** : Techniques to increase diversity in synthetic datasets\n3. **Practical Implementations** : Some simple examples of how to increase entropy and diversity to get better synthetic data\n\n## Using the same prompt for all experiments\n\nMany practitioners rely on straightforward API calls to Large Language Models (LLMs) for synthetic data generation. This approach often leads to unexpected homogeneity.\n\nWe can see an example below with `instructor` where we try to generate a list of random names using structured extraction. Note the use of a few validators to ensure the names are single words.\n\n```python\nimport instructor\nimport openai\nfrom tqdm.asyncio import tqdm_asyncio as asyncio\nfrom asyncio import run, Semaphore\nfrom pydantic import BaseModel, field_validator\n\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass Person(BaseModel):\n    name: str\n\n    @field_validator(\"name\")\n    def validate_name(cls, v) -> str:\n        if \"random\" in v.lower():\n            raise ValueError(\n                f\"Generate a valid name. {v} is not a valid name because it contains the name random\"\n            )\n\n        if len(v.split()) > 1:\n            return v.split()[0]\n\n        return v\n\n\nasync def generate_random_name(sem: Semaphore):\n    async with sem:\n        return await client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            response_model=Person,\n            messages=[{\"role\": \"user\", \"content\": \"Generate a random name\"}],\n        )\n\n\nasync def main(n_samples: int):\n    sem = Semaphore(10)\n    coros = [generate_random_name(sem) for _ in range(n_samples)]\n    names = await asyncio.gather(*coros)\n    names = [n.name for n in names]\n    print(len(set(names)))\n\n\nif __name__ == \"__main__\":\n    run(main(100))\n\n    #> 5\n    #> {'Alex', 'Alice', 'Jane', 'Avery', 'John', 'Randall', 'Harry', 'Jonathan'}\n```\n\nWe made 100 API calls and got 5 unique names. This is a good example of how simple generation methods can lead to unexpected homogeneity. How can we make this better?\n\n## Entropy\n\nA simple method here is to introduce some form of entropy. This simply means changing the prompt slightly for each call so that we have a more diverse set of prompts that we're using. This helps us to increase the diversity of the synthetic data.\n\nIn this case, we're using the `faker` library to generate a random country for each name. We then add in a randomly chosen age and gender to the prompt so that we get a more diverse set of names.\n\n```python\nimport instructor\nimport openai\nfrom tqdm.asyncio import tqdm_asyncio as asyncio\nfrom asyncio import run, Semaphore\nfrom pydantic import BaseModel, field_validator\nfrom faker import Faker\nimport random\n\nfake = Faker()\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass Person(BaseModel):\n    first_name: str\n\n    @field_validator(\"first_name\")\n    def validate_name(cls, v) -> str:\n        if \"random\" in v.lower():\n            raise ValueError(\n                f\"Generate a valid name. {v} is not a valid name because it contains the name random\"\n            )\n\n        if len(v.split()) > 1:\n            return v.split()[0]\n\n        return v\n\n\nasync def generate_random_name(sem: Semaphore):\n    country = fake.country()\n    age = random.randint(18, 65)\n    gender = random.choice([\"male\", \"female\"])\n    async with sem:\n        return await client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            response_model=Person,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Generate a random first name for a {gender} who is from {country} that is {age} years old\",\n                }\n            ],\n        )\n\n\nasync def main(n_samples: int):\n    sem = Semaphore(10)\n    coros = [generate_random_name(sem) for _ in range(n_samples)]\n    names = await asyncio.gather(*coros)\n    names = [n.first_name for n in names]\n    print(len(set(names)))\n    print(set(names))\n\n\nif __name__ == \"__main__\":\n    run(main(100))\n    # > 89\n    # > {'Gábor', 'Amani', 'Sipho', 'Leyla', 'Anita', 'Ousmane', 'Brian', 'Amin', 'Lassane', 'Luca', 'Jaan', 'Georgi', 'Aaliyah', 'Aigul', 'Tanisha', 'Émilie', 'Liam', 'Fatou', 'Ana', 'Carmen', 'Jānis', 'Alfredo', 'Linda', 'Raimonds', 'Marta', 'Ala', 'Tane', 'Male', 'Mireille', 'Andreea', 'Somchai', 'Emily', 'Mamadu', 'Shane', 'José', 'Amadou', 'Ezekiel', 'Sophie', 'Jamal', 'John', 'Mark', 'Derek', 'Marc', 'Mário', 'Tiko', 'Mia', 'Siti', 'Khalil', 'Lukáš', 'Amina', 'María', 'Nermin', 'Sigrún', 'Faakau', 'Nisha', 'Kebede', 'Salma', 'Malu', 'Maja', 'Thato', 'Marina', 'Boris', 'Thabo', 'Mandlenkosi', 'DeAndre', 'Lucas', 'Dagný', 'Malo', 'Demos', 'Mykola', 'Ivan', 'Giulia', 'Aleksandar', 'Elena', 'Aroha', 'Jean', 'Youssef', 'Aman', 'Sofía', 'Maria', 'Mika', 'James', 'Miaraka', 'Ogechi', 'Sela', 'Viktor', 'Joon', 'Dante', 'Juliana'}\n```\n\nWe can see that with this one simple change, we've increased the diversity of the synthetic data. We've gone from 5 unique names to 89 unique names. This is a good example of how we can use entropy to increase the diversity of the synthetic data.\n\n## Practical Considerations\n\nOften times, I've found that there are a few levers that we can pull to increase the diversity of the synthetic data. These tend to be\n\n1. Including few shot examples : These are incredibly useful for increasing the diversity of the synthetic data. They help to guide the model to generate more diverse outputs by explicitly showing examples of recent generations or outputs we want.\n\n```python\nasync def generate_random_name(sem: Semaphore, names: list[str]):\n    async with sem:\n        return await client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            response_model=Person,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Generate a random name, Make sure that this name is different from the previous names we generated of {names} \",\n                }\n            ],\n        )\n```\n\nIf you're generating things like synthetic questions, you can even go one step further and ensure that the cosine similarity of the question compared to previous generations is below a certain threshold. This can be done using simple validator here to ensure that the questions are diverse with the help of a validation context.\n\n```\nclass SyntheticQuestion(BaseModel):\n    question: str\n    answer: str\n\n    @field_validator(\"question\")\n    def check_similarity(cls, v):\n        # This is pseudo code\n        prev_questions = get_prev_questions()\n        if prev_questions and any(cossim(prev_q, v) > 0.5 for prev_q in prev_questions):\n            raise ValueError(\n                f\"Generate a unique question-answer pair. '{v}' is too similar to previously generated questions\"\n            )\n        return v\n```\n\n2. Changing the response model : In this case, we're using a structured extraction response model to generate a more diverse set of names. We've added a new backstory field to the response model so that the model has some diversity of output as it generates the names.\n\n```python\nclass Person(BaseModel):\n    backstory: str\n    first_name: str\n\n    @field_validator(\"first_name\")\n    def validate_name(cls, v) -> str:\n        if \"random\" in v.lower():\n            raise ValueError(\n                f\"Generate a valid name. {v} is not a valid name because it contains the name random\"\n            )\n\n        if len(v.split()) > 1:\n            return v.split()[0]\n\n        return v\n```\n\n3. Having some sort of chained generation process : This could look like generating a backstory for a person and then using that backstory to generate a name. This can help to increase the diversity of the synthetic data.\n\n```python\nclass CharacterProfile(BaseModel):\n    background: str\n    occupation: str\n    personality_traits: list[str]\n\nclass Character(BaseModel):\n    profile: CharacterProfile\n    name: str\n\nasync def generate_character(sem: Semaphore):\n    async with sem:\n        profile = await client.chat.completions.create(\n            model=\"gpt-4\",  # Note: Use an appropriate, available model\n            response_model=CharacterProfile,\n            messages=[{\"role\": \"user\", \"content\": \"Generate a character profile\"}],\n        )\n        character = await client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            response_model=Character,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Generate a name for a character with this profile: {profile}\",\n                }\n            ],\n        )\n        return character\n```\n\n## Conclusion\n\nGenerating high-quality synthetic data requires moving beyond simple API calls. By introducing entropy, leveraging few-shot examples, modifying response models, and implementing chained generation processes, you can significantly improve the diversity and quality of your synthetic datasets.\n\nRemember to always validate your synthetic data against your specific use case and adjust these techniques as needed. With these approaches, you'll be better equipped to create synthetic data that truly adds value to your AI development process.\n",
  "slug": "how-to-create-synthetic-data-that-works"
}