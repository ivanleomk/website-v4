{
  "title": "Getting Started with Evals - a speedrun through Braintrust",
  "date": "2024-09-05T00:00:00.000Z",
  "description": "Writing your first eval test in under 5 minutes",
  "categories": [
    "Evals",
    "Braintrust"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\nFor software engineers struggling with LLM application performance, simple evaluations are your secret weapon. Forget the complexity — we'll show you how to start testing your LLM in just 5 minutes using Braintrust. By the end of this article, you'll have a working example of a test harness that you can easily customise for your own use cases.\n\nWe'll be using a cleaned version of the GSM8k dataset that you can find [here](https://huggingface.co/datasets/567-labs/gsm8k).\n\nHere's what we'll cover:\n\n1. Setting up Braintrust\n2. Writing our first task to evaluate an LLM's response to the GSM8k with Instructor\n3. Simple recipes that you'll need\n\n## Why bother evaluating?\n\nEvaluations are key to building a great application. While we want to ship fast to prpduction and put features into the hands of our users, It's important to have tests so that we know how changes in our prompts, model choices and even the response object is affecting the output that we get.\n\nAdditionally, by knowing where our model fails, we can build and invest in systems that can mitigate a lot of these issues.\n\n## Setting up a Braintrust Evaluation Suite\n\nYou'll need a braintrust account to run the remainder of the code. You can sign up [here](https://www.braintrust.dev/) for a free account.\n\nLet's first start by installing the dependencies we'll need.\n\n```bash\nuv pip install braintrust autoevals instructor openai datasets\n```\n\nOnce we've done so, we can just go ahead and copy the following code into a file called `eval.py`.\n\nWhat this tiny snippet of code does is that\n\n1. **Task** : For each individual dictionary in the `data` list, it will call the `task` function with the `input` field as the argument.\n2. **ExactMatch** : ExactMatch compares the LLM's output to a predefined correct answer, returning a 1 or 0 for each individual example. This means that when we add it as a score, it will compare the response obtained from the `task` function with the `expected` field in the dictionary and give us a score.\n\n```python\nfrom braintrust import Eval\nfrom autoevals.value import ExactMatch\n\n\ndef task(input):\n    return \"Hi \" + input\n\n\nEval(\n    \"braintrust-tutorial\",  # Replace with your project name\n    data=lambda: [\n        {\n            \"input\": \"Foo\",\n            \"expected\": \"Hi Foo\",\n        },\n        {\n            \"input\": \"Bar\",\n            \"expected\": \"Hello Bar\",\n        },\n    ],  # Replace with your eval dataset\n    task=task,  # Replace with your LLM call\n    scores=[ExactMatch],\n)\n```\n\nBecause the second assertion `Hello Bar` won't match the response we get from `task`, we're only going to get a score of 50% here. This is expected since our function is hardcoded to return `hi <input>`.\n\nLet's run this code using `python eval.py`.\n\n```bash\n> python eval.py\nExperiment <> is running at <>\nbraintrust-tutorial (data): 2it [00:00, 4583.94it/s]\nbraintrust-tutorial (tasks): 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 30.93it/s]\n\n=========================SUMMARY=========================\n50.00% 'ExactMatch' score\n\n0.00s duration\n\nSee results for <> at https://www.braintrust.dev/<>\n```\n\nNow that we've got this setup, let's try looking at how we can convert this to evaluate the GSM8k dataset.\n\n## Writing our first task to evaluate an LLM's response to the GSM8k\n\nAs mentioned above, we'll be using a cleaned version of the GSM8K dataset that you can find [here](https://huggingface.co/datasets/567-labs/gsm8k). I processed this dataset so that it contains a nice answer field for each question. This means that each row looks something like this.\n\n```json\n{\n  \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n  \"answer\": \"72\",\n  \"reasoning\": \"Natalia sold 48/2 = <<48/2=24>>24 clips in May. Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\"\n}\n```\n\nWhat we want to do is to essentially give `gpt-4o-mini` the question, and then ask it to produce the answer and the reasoning that led to the answer.\n\nTo do this, we'll be using the `instructor` package that we installed.\n\n```python\nfrom braintrust import Eval\nfrom autoevals.value import ExactMatch\nfrom datasets import load_dataset\nimport instructor\nimport openai\nfrom pydantic import BaseModel\n\n# First we take the first 10 examples from the gsm8k dataset\nds = load_dataset(\"567-labs/gsm8k\", split=\"test\").take(10)\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef task(question: str):\n    class Answer(BaseModel):\n        chain_of_thought: str\n        answer: int\n\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=Answer,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that solves math problems. You are given a question and you need to solve it. You need to provide the answer and the steps to solve the problem.\",\n            },\n            {\"role\": \"user\", \"content\": question},\n        ],\n    ).answer\n\n\nEval(\n    \"braintrust-tutorial\",  # Replace with your project name\n    data=lambda: [\n        {\n            \"input\": row[\"question\"],\n            \"expected\": row[\"answer\"],\n        }\n        for row in ds\n    ],\n    task=task,  # Replace with your LLM call\n    scores=[ExactMatch],\n    metadata={\"dataset\": \"gsm8k\", \"split\": \"test\", \"n_examples\": 10},\n)\n```\n\nWhen we run this code, we should see an output that looks something like this.\n\n```bash\n> python eval.py\nExperiment <> is running at <>\nbraintrust-tutorial (data): 10it [00:00, 17389.32it/s]\nbraintrust-tutorial (tasks): 100%|████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.12s/it]\n\n=========================SUMMARY=========================\n---\n100.00% (+100.00%) 'ExactMatch' score   (10 improvements, 0 regressions)\n\n4.29s (+12.34%) 'duration'      (7 improvements, 3 regressions)\n\nSee results for <> at https://www.braintrust.dev/app/567/p/braintrust-tutorial/experiments/<>\n```\n\nThis is a good score and our LLM is performing well. Better yet, with Braintrust, we're able to capture the entire evaluation suite from start to finish.\n\n![](./images/braintrust_experiments.png)\n\nHowever, if we try to scale this up we're going to run into some problems. The main problem here is that we're not able to run our evaluations asynchronously and we're running each call sequentially.\n\nThankfully the answer isn't very complicated. All we need to do is to change our client to the `AsyncOpenAI` client, our task to an asynchronous function and then make a few small adjustments to the `Eval` function.\n\nWe can try running the code snippet below and we'll see that the code runs a lot faster.\n\n```python\nfrom braintrust import Eval\nfrom autoevals.value import ExactMatch\nfrom datasets import load_dataset\nimport instructor\nimport openai\nfrom pydantic import BaseModel\nimport asyncio\n\n# First we take the first 10 examples from the gsm8k dataset\nds = load_dataset(\"567-labs/gsm8k\", split=\"test\").take(30)\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nasync def task(question: str):\n    class Answer(BaseModel):\n        chain_of_thought: str\n        answer: int\n\n    return (\n        await client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            response_model=Answer,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant that solves math problems. You are given a question and you need to solve it. You need to provide the answer and the steps to solve the problem.\",\n                },\n                {\"role\": \"user\", \"content\": question},\n            ],\n        )\n    ).answer\n\n\nasync def main():\n    await Eval(\n        \"braintrust-tutorial\",\n        data=lambda: [\n            {\n                \"input\": row[\"question\"],\n                \"expected\": row[\"answer\"],\n            }\n            for row in ds\n        ],\n        task=task,\n        scores=[ExactMatch],\n        metadata={\"dataset\": \"gsm8k\", \"split\": \"test\", \"n_examples\": 30},\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nWe can see that the code works just as well, but it runs a lot faster. Better yet, we're able to also capture all of the generated results in the UI as seen below.\n\n![](./images/braintrust_async_experiments.png)\n\n## Quick Tips\n\nLastly, I'll leave you with a few quick tips to help make better use of Braintrust.\n\n1. Use the hooks method in Braintrust to log arbitrary information. I've found this to be very useful when you go back and try to understand why a certain model performed better than another. Better yet, having the data captured as is, helps significantly with debugging and finding difficult/challenging edge cases.\n\n```python\nasync def task(question: str, hooks):\n    class Answer(BaseModel):\n        chain_of_thought: str\n        answer: int\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that solves math problems. You are given a question and you need to solve it. You need to provide the answer and the steps to solve the problem.\",\n        },\n        {\"role\": \"user\", \"content\": question},\n    ]\n    resp = await client.chat.completions.create(\n        model=model,\n        response_model=Answer,\n        messages=messages,\n    )\n    hooks.meta(response_object=resp.model_dump_json(indent=2), messages=messages)\n    return resp.answer\n\n```\n\n2. Try to parameterize different aspects of your evaluation and log as much as you can. Here's an example where we parameterize the model that we eventually end up using in tasks so that we can log it in the experiment metadata.\n\n```python\nmodel = \"gpt-4o-mini\"\n\n#  Task code goes here\n\nasync def main():\n    await Eval(\n        \"braintrust-tutorial\",\n        data=lambda: [\n            {\n                \"input\": row[\"question\"],\n                \"expected\": row[\"answer\"],\n            }\n            for row in ds\n        ],\n        task=task,\n        scores=[ExactMatch],\n        metadata={\n            \"dataset\": \"gsm8k\",\n            \"split\": \"test\",\n            \"n_examples\": 30,\n            \"model\": model,\n        },\n    )\n```\n\n3. Write custom scorer functions - these are useful in helping to capture functionality that isn't available in the autoevals library. The following example shows how we can calculate MRR and Recall at different values of `k` in Braintrust. This is extremely useful when we want to parameterize our evaluation to run different metrics or different sizes of metrics.\n\n```python\n\ndef calculate_mrr(predictions: list[str], gt: list[str]):\n    mrr = 0\n    for label in gt:\n        if label in predictions:\n            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n    return mrr\n\n\ndef get_recall(predictions: list[str], gt: list[str]):\n    return len([label for label in gt if label in predictions]) / len(gt)\n\n\neval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", get_recall]]\nsizes = [3, 5, 10, 15, 25]\n\nmetrics = {\n    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n        lambda p, g: m(p[:s], g)\n    )(predictions, gt)\n    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n}\n\ndef evaluate_braintrust(input, output, **kwargs):\n    return [\n        Score(\n            name=metric,\n            score=score_fn(output, kwargs[\"expected\"]),\n            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n        )\n        for metric, score_fn in metrics.items()\n    ]\n```\n\nWe can see here that with this method, we're able to log all of our custom metrics. In Braintrust, we'll need to make sure that our scores all lie within the `Score` class so that we can capture them properly. We can then call this function using the high level `Eval` function as seen below.\n\n```python\nEval(\n    \"MS Marco\",  # Replace with your project name\n    data=lambda: [\n        {\n            \"input\": query,\n            \"expected\": label,\n            \"metadata\": {\"search_type\": \"fts\", \"k\": \"25\"},\n        }\n        for query, label in zip(queries, labels)\n    ],  # Replace with your eval dataset\n    task=lambda query: [\n        row[\"chunk\"]\n        for row in table.search(query, query_type=\"fts\")\n        .select([\"chunk\"])\n        .limit(25)\n        .to_list()\n    ],\n    scores=[evaluate_braintrust],\n    trial_count=3,\n)\n```\n\n4. Use the `task` method as a way to run calls, this is great when your evaluation logic requires more than just the `input` parameter.\n\n```python\ndef task_scoring_function(input, arg1, arg2, arg3):\n    return compute_something(input, arg1, arg2, arg3)\n\n\n\ndef task(input):\n    return task_scoring_function(input, arg1, arg2, arg3)\n\nawait Eval(\n    ...\n    task=task, # This is an easy way if we have a bunch of different parameterized tests that we'd like to\n)\n```\n\n5. When running a set of Evals, make sure to save the resulting `Result` object that's returned to you. This is extremely useful when you want to see all of the results in a single place.\n\n```python\nresult = Eval(\n    \"MS Marco\",  # Replace with your project name\n    data=lambda: [\n        {\n            \"input\": query,\n            \"expected\": label,\n            \"metadata\": {\"search_type\": \"fts\", \"k\": \"25\"},\n        }\n        for query, label in zip(queries, labels)\n    ],  # Replace with your eval dataset\n    task=lambda query: [\n        row[\"chunk\"]\n        for row in table.search(query, query_type=\"fts\")\n        .select([\"chunk\"])\n        .limit(25)\n        .to_list()\n    ],\n    scores=[evaluate_braintrust],\n    trial_count=3,\n)\n\n# This can be saved into an array for you to print out at the end of your script\nexperiment_name = result.summary.experiment_name\nexperiment_url = result.summary.experiment_url\nscores = result.summary.scores\n\nsummarized_scores = [f\"{result.summary.scores[score].score:.2f}\" for score in scores]\n```\n\nI hope this helped significantly with getting started with Braintrust. If you have any questions, please feel free to reach out to me on twitter [@ivanleomk](https://twitter.com/ivanleomk). I struggled a fair bit when I was getting used to it so hopefully this helps!\n",
  "slug": "braintrust-from-scratch"
}