{
  "title": "A Weekend of Text to Image/Video Models",
  "date": "2024-12-22T00:00:00.000Z",
  "description": "Practical guide to setting up text-to-image and video generation using Modal and ComfyUI",
  "categories": [
    "Modal",
    "Diffusion Models",
    "ComfyUI"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n# A Weekend of Text to Image/Video Models\n\n> You can find the code for this post [here](https://github.com/ivanleomk/comfy-experiments).\n\nI had a lot of fun playing around with text to image models over the weekend and thought I'd write a short blog post about some of the things I learnt. I ran all of this on Modal and spent ~10 USD across the entire weekend which is honestly well below the Modal $20 free tier credit.\n\nThis was mainly for a small project i've been working on called CYOA where users get to create their own stories and have a language model automatically generate images and choices for each of them.\n\n## Model Capabilities and Examples\n\nI played around with 3 different models over the weekend - flux, ltx-video and mochi. At the end, I think I'll end up going with flux for the final version of CYOA's graphics since it produces the most consistently good results. I've found LTX-Video and Mochi to be a bit inconsistent ( it is after all much harder to get good results with video generations ).\n\nTo demonstrate these capabilities, here's an example using LTX-Video with a detailed prompt:\n\n```\nClose-up portrait of an 80-year-old fisherman, shot on Sony Î±7 III, 85mm f/1.2 lens,\ndramatic side lighting, deep wrinkles telling stories of ocean adventures, weathered\nskin with salt-and-pepper beard, piercing blue eyes looking off-frame, wearing a\nfaded denim work jacket, soft blurred background of misty coastline\n```\n\n![LTX Video](./images/ComfyUI_00004_-2.webp)\n\nI generally find that prompt quality has a huge bearing on output. Most scenes require multiple iterations of prompt refinement to achieve the result that you want. Ultimately, I think this will be a manual process for the user to do and iterate until you get something that you want.\n\n## Why ComfyUI?\n\nI think you should use ComfyUI over rolling your own complex workflow code at the start for 3 main reasons.\n\n1. It's quick to get started\n2. It's easy to download a workflow that someone has created and use that as a starting point\n3. Many models come out of the box with `ComfyUI` support that you don't need to implement youself - that's a huge time saver. In fact LTX only can be used with ComfyUI or their own bespoke repository code.\n\nThis means that while you can definitely do the following to generate images with flux and get a slight performance improvement, ComfyUI is a much better starting point, especially if you're a beginner to prompting text to image/video models like I am. Personally I struggled to get many of these models working nicely, and at the start, couldn't find a way to get around Flux's 75 token limit until ComfyUI made it easy to hook in the `t5` encoder to swop out the `clip` encoder.\n\n```python\npipe = FluxPipeline.from_pretrained(\n    f\"black-forest-labs/FLUX.1-{VARIANT}\",\n    torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\nresults = pipe(\n    prompts,\n    output_type=\"pil\",\n    num_inference_steps=NUM_INFERENCE_STEPS,\n    width=800,\n    height=400,\n)\n```\n\nMore importantly, by using ComfyUI we can easily put together complex workflows and visually debug the results. Changing the prompt or model itself is as easy as clicking a button - which is a huge time saved when tinkering between say flux-dev and flux-schnell for example to see which makes the most sense for your use case.\n\nThese workflows are a bit clunky in `.json` as you'll see below so I highly recommend using the UI to create them by exporting them.\n\n```json\n{\n  \"5\": {\n    \"inputs\": {\n      \"width\": 1024,\n      \"height\": 1024,\n      \"batch_size\": 1\n    },\n    \"class_type\": \"EmptyLatentImage\"\n  }\n}\n```\n\n## Setting Up with Modal\n\nThe implementation consists of three main components working together. First, we need efficient model weight management for downloading and storing the models. Then, we'll create a development UI for interactive experimentation. Finally, we'll set up a production endpoint for scalable deployment.\n\n### Model Weight Management\n\nWe start by creating a Modal Image with the necessary dependencies. This setup enables parallel downloading and efficient storage of model weights:\n\n```python\nimport modal\n\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .pip_install(\"huggingface_hub[hf_transfer]==0.26.2\")\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n    .run_commands(\"rm -rf /root/comfy/ComfyUI/models\")\n)\n\napp = modal.App(name=\"comfyui-models\", image=image)\nvol = modal.Volume.from_name(\"comfyui-models\", create_if_missing=True)\n```\n\nTo handle the downloads efficiently, we implement a parallel download function:\n\n```python\n@app.function(\n    volumes={\"/root/models\": vol},\n    secrets=[modal.Secret.from_name(\"my-huggingface-secret\")],\n)\ndef hf_download(repo_id: str, filename: str, model_type: str):\n    from huggingface_hub import hf_hub_download\n    print(f\"Downloading {filename} from {repo_id} to {model_type}\")\n    hf_hub_download(\n        repo_id=repo_id,\n        filename=filename,\n        local_dir=f\"/root/models/{model_type}\",\n    )\n```\n\nWe can then use this function to download the models we need.\n\n```python\n@app.local_entrypoint()\ndef download_models():\n    models_to_download = [\n        # format is (huggingface repo_id, the model filename, comfyui models subdirectory we want to save the model in)\n        (\n            \"black-forest-labs/FLUX.1-schnell\",\n            \"ae.safetensors\",\n            \"vae\",\n        ),\n        (\n            \"black-forest-labs/FLUX.1-schnell\",\n            \"flux1-schnell.safetensors\",\n            \"unet\",\n        ),\n        ...// define more models here\n    ]\n    list(hf_download.starmap(models_to_download))\n\n```\n\n### Development UI\n\nFor the development phase, we create an interactive ComfyUI instance. This interface allows us to experiment with different models and parameters in real-time:\n\n```python\n@app.function(\n    allow_concurrent_inputs=10,\n    concurrency_limit=1,\n    container_idle_timeout=30,\n    timeout=1800,\n    gpu=\"A100\",\n    volumes={\"/root/comfy/ComfyUI/models\": vol},\n)\n@modal.web_server(8000, startup_timeout=60)\ndef ui():\n    subprocess.Popen(\n        \"comfy launch -- --listen 0.0.0.0 --port 8000\",\n        shell=True\n    )\n```\n\nAccess the development interface by running `modal serve ui.py`. This provides an interactive environment for testing and refining your generation pipelines.\n\n### Production Endpoint\n\nFor production deployment, we create a robust API endpoint that can handle multiple requests efficiently:\n\n```python\n@app.cls(\n    gpu=\"A100\",\n    mounts=[\n        modal.Mount.from_local_file(\n            Path(__file__).parent / \"flux.json\",\n            \"/root/flux.json\",\n        )\n    ],\n    volumes={\"/root/comfy/ComfyUI/models\": vol},\n)\nclass ComfyUI:\n    @modal.enter()\n    def launch_comfy_background(self):\n        cmd = \"comfy launch --background\"\n        subprocess.run(cmd, shell=True, check=True)\n\n    @modal.web_endpoint(method=\"POST\")\n    def api(self, item: Dict):\n        from fastapi import Response\n\n        workflow_data = json.loads(\n            (Path(__file__).parent / \"flux.json\").read_text()\n        )\n        workflow_data[\"6\"][\"inputs\"][\"text\"] = item[\"prompt\"]\n        client_id = uuid.uuid4().hex\n        workflow_data[\"9\"][\"inputs\"][\"filename_prefix\"] = client_id\n\n        new_workflow_file = f\"{client_id}.json\"\n        json.dump(workflow_data, Path(new_workflow_file).open(\"w\"))\n        img_bytes = self.infer.local(new_workflow_file)\n\n        return Response(img_bytes, media_type=\"image/jpeg\")\n```\n\nThe performance characteristics reveal the system's efficiency. Cold starts take approximately 90 seconds as the model loads into memory. Once warm, requests complete in 5-10 seconds. The system scales horizontally when multiple A100s are available, making it suitable for production workloads.\n\n## Improving Output Quality\n\n### Prompt Engineering with LLMs\n\nThrough experimentation, I found that using language models to generate scene descriptions dramatically improves output quality. These LLM-generated prompts incorporate camera specifications, lighting details, visual references, and scene elements in a coherent way.\n\n```markdown\nGenerate a scene description (~100 words) including:\n\n- Reference shots from specific films\n- Detailed scene elements and composition\n- Technical camera details\n\nHere are some good examples\n\n- The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.\n\n<story>\n\n</story>\n```\n\nThe resulting prompts provide rich detail for the image generation models to work with, leading to more consistent and higher quality outputs.\n\n### Video Output Processing\n\nWorking with video outputs requires careful handling of formats and conversions. I developed this utility function to manage the process:\n\n```python\nimport requests\nfrom PIL import Image\n\ndef generate_and_convert_video(modal_url, prompt):\n    # API request\n    response = requests.post(modal_url, json={\"prompt\": prompt})\n\n    if response.status_code == 200:\n        # Save WebP\n        with open(\"output.webp\", \"wb\") as f:\n            f.write(response.content)\n\n        # Convert to GIF\n        im = Image.open(\"output.webp\")\n        frames = []\n        try:\n            while True:\n                frames.append(im.copy())\n                im.seek(len(frames))\n        except EOFError:\n            pass\n\n        if frames:\n            frames[0].save(\n                \"output.gif\",\n                format=\"GIF\",\n                append_images=frames[1:],\n                save_all=True,\n                duration=im.info.get(\"duration\", 100),\n                loop=1\n            )\n```\n\nThis utility handles the complete pipeline from generation request to final GIF output, making it simple to integrate video generation into larger applications.\n\n## Conclusion\n\nModal and ComfyUi makes it easy to get started with complex stable diffusion-esque workflows. Instead of worrying about the infrastructure or spending hours making sure your workflows are working nicely, you can just click and try different models and workflows without much difficulty. Deploying/scaling it is also a breeze.\n\nWhen you get to a specific scale, then maybe worry about the pipeline and the performance differential that using ComfyUi might cost you. But until then, I think it's probably the dominant way that I'll be using these models from now on.\n",
  "slug": "a-weekend-of-text-to-image-models"
}