{
  "title": "Why you're missing out on not building with streaming UIs in mind",
  "date": "2025-03-25T00:00:00.000Z",
  "description": "Key considerations when building UIs that rely on streaming LLM content",
  "categories": [
    "UI/UX",
    "LLMs"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n# Use streaming UIs and customers are willing to wait 5x longer\n\nIf you're not building your application with streaming in mind, you're making two major mistakes\n\n1. You're going to have to spend months refactoring your code to adapt to streaming once you make this decision\n2. You're missing out on a major opportunity to improve the user experience.\n\nI've built a fair bit of user interaces that rely on streaming content with tools like the [`ai-sdk` by `vercel`](https://sdk.vercel.ai/docs/introduction) and I've found that there are three main considerations to think about when building out your application.\n\nIn this article we'll cover\n\n1. Why streaming becomes increasingly important\n2. You need to think carefully about the type of streaming content you're relying on - whether you can use intermediate states, require a complete object or are trying to chain together a series of streaming LLM calls.\n3. Some practical techniques for smooth animations and layouts\n\nHere's a great example of how streaming can help make a UI seem much more responsive that I coded up over the weekend.\n\n![](https://r2-workers.ivanleomk9297.workers.dev/generative_ui.mp4)\n\n## Why care about streaming?\n\nAs we move towards more complex LLM applications, response times are increasing significantly. This is especially true as we start using reasoning models more heavily that will spend a long time spinning on their wheels to reason through different approaches. This means that users will start to deal with longer waiting periods before seeing a response to their question.\n\nBy implementing proper streaming, we can transform a frustrating waiting period into an interactive experience. By seeing immediate feedback, we provide a responsive feedback and make a long operation feel more engaging. By starting with streaming interfaces from the get-go, we can help avoid a world of pain when we have to migrate back and implement support for intermediate states and other intricacies around streaming.\n\n## What Type Of Streaming Content are you using?\n\nIn general, I tend to find that there are three kinds of streaming UIs that I've seen out in the wild. These are\n\n1. Formatted responses where we have some sort of chatbot response streamed in as markdown\n2. Dependent API Calls where we might want to chain different LLM api calls together - Eg. we get cheaper LLMs to pre-process some information or query then get a reasoning LLM to generate a final response\n3. Iterable Objects - Where we might want to stream out UI content based on specific fields or objects that need to be completely generated before we can take downstream action\n\nLet's take a look at each of them.\n\n### 1. Formatted Responses\n\nThe simplest form of streaming is when we display markdown or formatted text directly to users. However, getting consistent formatting requires careful consideration of both prompting and rendering.\n\nFor reliable formatting:\n\n```typescript\nconst { completion } = useCompletion({\n  api: \"/api/stream/content\",\n  system: `Always structure your response with:\n  1. A main title using # \n  2. Clear paragraph breaks with blank lines\n  3. Consistent header hierarchy`,\n});\n\n// Use React Markdown for consistent rendering\nreturn (\n  <ReactMarkdown\n    components={{\n      // Custom components for better formatting control\n      h1: ({ children }) => (\n        <h1 className=\"text-2xl font-bold mb-4\">{children}</h1>\n      ),\n      p: ({ children }) => <p className=\"mb-4 leading-relaxed\">{children}</p>,\n    }}\n  >\n    {completion}\n  </ReactMarkdown>\n);\n```\n\nNote that with something like React Markdown we can also provide the model with the ability to generate custom components using simple markdown and IDs with features such as XML tags which are incredibly useful.\n\nI implemented an example [here](https://github.com/ivanleomk/xml-experiments) that shows how to implement this with react markdown for custom components to display chain of thought and citations using XML tags such as `<citations>` and `<thinking>`.\n\n![](https://r2-workers.ivanleomk9297.workers.dev/citations.mp4)\n\nYou can enhance the user experience during generation by adding interstitial states:\n\n```typescript\nfunction StreamingResponse({ completion, isLoading }) {\n  return (\n    <div>\n      {completion}\n      {isLoading && !completion && (\n        <div className=\"text-gray-500 italic\">\n          {getRandomThinkingMessage()} // \"Pondering...\", \"Analyzing...\"\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n### 2. Chained API Calls\n\nFor more complex applications, we often need to chain multiple LLM calls together. This might involve using cheaper models for initial processing before engaging more expensive models for final reasoning.\n\n```typescript\nconst { completion, isLoading } = useCompletion({\n  api: \"/api/initial-analysis\",\n  onFinish: (result) => {\n    // Chain to more expensive model for deeper analysis\n    completeWithExpensiveModel({\n      context: result.completion,\n    });\n  },\n});\n```\n\n### 3. Processing Iterable Objects\n\nThe most complex case involves streaming UI components that depend on specific fields being complete. Models stream fields in different orders - OpenAI follows schema order while Gemini uses alphabetical order (unless configured otherwise).\n\nHere's how to handle this with a content carousel:\n\n```typescript\nconst ContentCarousel = ({ items, isLoading }) => {\n  // Don't render until we have enough items\n  if (!items || (isLoading && items.length <= 1)) {\n    return <LoadingSkeleton />;\n  }\n\n  return (\n    <div className=\"carousel\">\n      {items.map((item, index) => {\n        // Check required fields based on streaming order\n        // OpenAI streams in schema order, so if title exists\n        // we know previous fields are complete\n        if (!item?.title) {\n          return null;\n        }\n\n        return (\n          <div className=\"w-full h-[400px]\" key={index}>\n            {/* Only render image once title/description ready */}\n            {item.title && item.description && (\n              <img\n                src={`/api/images/${item.banner_slug}`}\n                className=\"w-full h-full object-cover\"\n              />\n            )}\n            <ContentDetails item={item} />\n          </div>\n        );\n      })}\n    </div>\n  );\n};\n```\n\n## Practical tips\n\nTo prevent layout shifts, establish clear dimensions early:\n\n```typescript\nfunction StreamingCard({ content }) {\n  return (\n    <div\n      className=\"min-h-[200px] w-full transition-all duration-300\"\n      style={{\n        // Set fixed initial dimensions\n        height: content ? \"auto\" : \"200px\",\n        maxHeight: \"600px\",\n      }}\n    >\n      {/* Content placeholder while streaming */}\n      {!content && (\n        <div className=\"animate-pulse bg-gray-200 h-full w-full rounded\" />\n      )}\n      {content}\n    </div>\n  );\n}\n```\n\nThis is something that I often see people not doing and it makes the UIs a bit jarring. I often forget too so this is a small reminder to myself too.\n\n## Conclusion\n\nIn conclusion, streaming UIs are no longer just a nice-to-have feature but a critical component of modern LLM-powered applications. By designing with streaming in mind from the start, developers can avoid painful refactoring later while dramatically improving user experience through immediate feedback and responsive interfaces.\n\nWhether you're working with formatted markdown responses, chained API calls, or complex iterable objects, implementing proper dimension constraints, thoughtful loading states, and careful field handling will ensure your UI feels polished and responsive even when processing complex LLM operations.\n\nIf you're experimenting or building UIs like this, would love to chat!\n",
  "slug": "thinking-about-streaming-ui"
}