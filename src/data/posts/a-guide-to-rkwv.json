{
  "title": "A guide to RWKV V3",
  "date": "2023-09-28T00:00:00.000Z",
  "description": "A guide to a strong open-source transformer alternative",
  "categories": [
    "LLMs",
    "RWKV"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n## Introduction\n\nRWKV is an alternative to the transformer architecture. It's open source and has it's own [paper](https://arxiv.org/abs/2305.13048) over here. I found out about it sometime back in a paper club and thought i'd write a short article about it with what I had learnt.\n\nHere are some other resources which you might find useful about RWKVs\n\n- [RKWV by Picocreator](https://github.com/PicoCreator/2nd-brain/blob/main/A%20-%20TechTalkCTO/P%20-%20RWKV%20Musings/The%20RWKV%20architecture%20-%20scaling%20RNN%20to%20transformer%20scale/RWKV%20architecture%20-%20Scaling%20an%20RNN%20to%20transformer%20scale.md) This is a markdown file that was used by one of the contributors - Picocreator to give a short presentation on the RWKV architecture.\n\n- [RKWV in 100 lines](https://johanwind.github.io/2023/03/23/rwkv_details.html) Which covers the implementation of RWKV in 100 lines of code. Much of this article is based off the content here - I try to extend and provide my own intuition for some proofs. I've also attached a [colab notebook](https://colab.research.google.com/drive/1ZRHKtJsYY8DSh09Mm2WH7iHayX7NUrMX?usp=sharing) for you if you want to play with the code.\n\nWith that being said, let's dive into RWKVs.\n\n> I'm using the 430M model here. Hence why my embedding space is 1024. Other models might differ so do take note if you are experimenting with the larger models.\n>\n> Do note that there are some issues with numerical instability in the simplified version provided by johanwind which have been fixed in the implementation in Blink's repo. **This is not production code**.\n\n## Background\n\n### Transformers\n\n> For a better and more thorough explanation, do check out [GPT in 60 Lines of\n> numpy](https://jaykmody.com/blog/gpt-from-scratch/) by Jay K mody. Some of the\n> content here is paraphrased and I suggest going to the source for the best\n> understanding.\n\nWhat is a transformer? Normally today, when we talk about transformers, we're refering to GPTs - Generative Pre-Trained Transformers. These are huge models that have billions of parameters and are trained on lots of data. This enables them to be able to have an underlying understanding of language.\n\nHow do they do this? You can think of a transformer as having the following function signature where\n\n- `n_seq`: This is the size of the initial data passed in. It has to be less than or equal to the context length.\n- `n_vocab`: Our transformer outputs a prob distribution for each character in the input + the new predicted output.\n\n```py\ndef gpt(inputs: list[int]) -> list[list[float]]: # inputs has shape [n_seq]\n# output has shape [n_seq, n_vocab]\n\toutput = # beep boop neural network magic\n\treturn output\n```\n\nA big part of this magic happens because of the self-attention mechanism, which gives transformers their special powers but also increases the time needed to make a prediction.\n\n### Attention and Why it sucks\n\n> Some parts of this explanation are from Code Emporium's Excellent explanation\n> on Transformers which you can view\n> [here](https://www.youtube.com/watch?v=TQQlZhbC5ps)\n\nThe most important to take away from this section is that Attention scales quadratically. That's the single biggest bottleneck when we want to scale transformers to do more things.\n\nLet's see a graphical representation of what that might look like\n\n![](./images/attention-equation.png)\n\n<!-- <img src=\"/images/attention_illustration.png\" alt=\"Attention\" /> -->\n\nA few things here to point out\n\n1. Each word never sees a word that's beyond it's position. In the context of GPT when we are doing next token prediction, we don't the model to \"see the future\"\n\n2. Every single word prior to a word is compared against it to determine its relevancy to predicting the next word.\n\nIn our example above,\n\n- What is compared to What (1 comparison )\n- Is is compared to What and Is ( 2 comparisons )\n- My is compared to What,Is and My ( 3 comparisons )\n- Name is compared to What, Is, My and Name ( 4 comparisons )\n\n> In some cases, (1) might not hold (Eg. in Translation) where grammar\n> structures differ between languages but for a GPT, this is normally the case.\n\nbecause (2) is happening, then this means that the number of comparisons for a string that has been broken down into `n` tokens is going to be\n\n$$\n1 + 2 + \\dots + n = \\frac{n(n+1)}{2}\n$$\n\nwhich is quadratic in nature. This is an oversimplification of attention but you get the idea. That's why it gets more and more costly as we expand our models context sizes because there are simply more comparisons to be made.\n\n> We cannot cache these previous lookups in a transformer. This is one of the biggest problems with Transformers that **RWKV doesn't have** as you'll see down below.\n\nIn transformers, the self-attention mechanism computes query, key, and value vectors for each word in the input sequence and uses the dot product of these vectors to determine the attention scores. This process is sensitive to the specific tokens in the input sequence, and as a result, caching the attention scores for one sequence may not be applicable to a subsequent sequence.\n\n## RWKV\n\nRWKVs aim to solve the issue of attention by approximating attention with a linear operation instead. This results in 1-1.5x cheaper training costs and around 10x cheaper inference costs, especially as the number of tokens passed into the model increase over time.\n\nWe can see this with the following graphic from the RWKV paper that shows how the inference cost grows with respect to the token count. This means that as the length of what we pass in increases, we enjoy significantly lower inference time with the RWKV architecture.\n\n![](./images/Scaling_RWKV.png)\n\n### High Level Understanding\n\nRWKVs operate using two main mechanisms\n\n1. A world state : This stores information on information such as previous computations\n2. Temporal mechanism : RWKV uses a decay function to reduce the weightage of past information w.r.t new information.\n\nHowever, as a result of a world state, the RWKV model has a major weakness - it might end up discarding content which is relevant but not explictly requested at the start of the prompt.\n\nAs a result, when we're prompting the RWKV model, we want to use the following syntax\n\n```\n{{INSTRUCTION}}\n\n{{CONTEXT}}\n\n{{ANSWER}}\n```\n\ninstead of the typical format of\n\n```\n{{CONTEXT}}\n\n{{INSTRUCTION}}\n\n{{ANSWER}}\n```\n\n## Architecture\n\n### Sigmoid\n\nIntuitively, the sigmoid function is used as a activation function in both the time-mixing and acts as a forget gate in our Time Mixing and Channel Mixing blocks. Since all values are effectively coerced from 0 to 1, A value closer to 0 means the information should be forgotten, while a value closer to 1 means the information should be retained.\n\nThis plays a crucial role in determining the relevance of information from prior steps, allowing the network to selectively retain or discard information based on the current input and previous hidden state\n\n### State\n\nIn essence, we can think of the state as being comprised of the following components\n\n`[layerState,layerState,.... ]`\n\nInside each layerState, we have a total of 4 subarrays\n\n`state = np.zeros((N_LAYER, 4, N_EMBD), dtype=np.float32)`\n\ninside the state, we allocate space for each element as\n\n- 0 : prevX computation\n- 1 : prevNum Computation\n- 2 : prevDen Computation\n- 3 : prevChannelMixing result\n\nThis helps our model to be able to get some concept of time. Note that (3) is mostly used to replace the short-term memory ( since it can only look back a single step )\n\n### Time-Mixing\n\nTime mixing approximates attention and can be likened to the LSTM component of a traditional RNN architecture. This is because it has access to two things\n\n- Previous World State\n- Learned weights to determine how to combine previous computations and new computations.\n\n> Intuitively as time `t` increases, then the vector is dependent on a long history and a summation of an increasing number of terms. This creates a memory which can keep track of information provided in an earlier context.\n\nWe can visualise the entire architecture of the time-mixing block as seen below.\n\n![](./images/Time_Mixing_Block.png)\n\nLet's look at a function to compute time-mixing\n\n```python\n# Time mix layer with the various params\ndef time_mixing(\n\t\t# Incoming state of the current token\n\t\tx,\n\t\t# Previous token shift state\n\t\tlast_x,\n\t\t# Previous state, split across 2 values to prevent overflows\n\t\tlast_num, last_den,\n\t\t# Various weights, all trainable\n\t\tdecay, bonus, mix_k, mix_v, mix_r, Wk, Wv, Wr, Wout\n\t):\n\n\t# Given the incoming state, and the previous token shift state\n\t# compute R,K,V values. The `x * mix + last * (1-mix)` pattern\n\t# helps the model have trained weights to decide which factors\n\t# it wants to use for the respective process\n    k = Wk @ ( x * mix_k + last_x * (1 - mix_k) )\n    v = Wv @ ( x * mix_v + last_x * (1 - mix_v) )\n\n\t# Since R is used for the final gating of output (similar to attention score)\n\t# you can view this as a lite form of Q @ K\n    r = Wr @ ( x * mix_r + last_x * (1 - mix_r) )\n\n\t# Here we effectively do magic(last_state + k) * v\n\t#\n\t# But in essence, its the \"summation with decay\" of all the past expotents\n\t# divided by another set of expotents for numeric stability\n\t# (aka anti exploding gradients).\n\t#\n\t# Bonus is used to boost the signal for the WKV value\n    wkv = (last_num + exp(bonus + k) * v) / \\\n          (last_den + exp(bonus + k))\n\n\t# We compute the cumulative sum, for the next round, where it\n\t# is stored and forwarded as a state, with a gradual\n\t# decay value on the previous value.\n\t#\n\t# `exp(-exp(decay))` is essentialy a math hack to ensure decay\n\t# is between 0-1, and will gradually fade out past value if desired\n\t#\n\t# `exp(k) / exp(k) * v` is the summation accumulation of the current state\n\t# to be used in the next time mix\n    num = exp(-exp(decay)) * last_num + exp(k) * v\n    den = exp(-exp(decay)) * last_den + exp(k)\n\n\t# sigmoid then acts looseley as both a part of Q in QKV,\n\t# and as a forget gate in LSTM (together with decay)\n\t# for the WKV values\n    rwkv = sigmoid(r) * wkv\n\n\t# And finally that gets normalized into an output, and next state\n    return Wout @ rwkv, (x,num,den)\n```\n\nThis function implements the following equations of\n\n![](./images/time_mixing_eqn.png)\n\nIn our function, We have the parameters of\n\n- `last_x,last_num, last_den` : These are all stored in a huge `(N_Layers, 4,N_embeddings)` array and simply store the previous computations for each layer.\n\n  - `last_x`: $x_{t-1}$\n  - `last_num`: $wkv_{t-1}$ numerator\n  - `last_den`: $wkv_{t-1}$ denominator\n\n- `decay, bonus, mix_k, mix_v, mix_r, Wk, Wv, Wr, Wout`\n  - `decay` : $w$ parameter can be treated as $e^{-\\text{decay}}$\n  - `bonus` : This is equivalent to the `u` parameter above\n  - `mix_k`,`mix_r` and `mix_v` are equal to $\\mu_k,\\mu_r,\\mu_t$\n  - `Wk,Wv,Wr and wout` are equivalent to $W_k,W_v,W_r$ and $W_o$ above\n- Note here that $\\sigma$ simply represents the sigmoid activation function.\n\nIn the code above, it might be a little bit difficult to visualise the dimensions so\n\n- (1024,1024) : $W_r, W_k, W_v$\n- (1024, ): $\\mu_r,x_t,x_{t-1},\\mu_k,\\mu_v$ , decay, bonus\n\nThis means that essentially $r_t,k_t$ and $v_t$ are all going to be of the dimension `(1024,)` and the final output of this function will be of `(1024,)`.\n\n> What's interesting about this is that information on all previous tokens seen\n> is stored in a 1024 dimensional array. Compare this to transformers that\n> generate a `n_seq x n_embd` array due to the attention mechanism. Is attention\n> truly even needed?\n\n#### Inductive Proof\n\nI was slightly confused by these 4 lines of code since I couldn't quite understand the leap from last_num to num and den to last_den.\n\n```py\nwkv = (last_num + exp(bonus + k) * v) /      \\\n          (last_den + exp(bonus + k))\nrwkv = sigmoid(r) * wkv\n\nnum = exp(-exp(decay)) * last_num + exp(k) * v\nden = exp(-exp(decay)) * last_den + exp(k)\n```\n\nWhich tries to implement the step of\n\n$$\nwkv_t=\\frac{\\sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i}v_{i} + e^{\\mu+k_t}v_t}{\\sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i} + e^{\\mu+k_t}}\n$$\n\nLet's try to build the intuition for this step by step. Let's say we wanted to take the sum of all the elements in the following sum. ( This is present in the numerator ).\n\n$$\n\\sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i}v_{i}\n$$\n\nLet's also designate the sum of all the elements defined in the sum as $\\alpha_{t-1}$ where we are summing elements from 1 to $t-1$. How do we get from $\\alpha_{t-1}$ to $\\alpha_{t}$?\n\nIf we look at the term $\\alpha_{t-1}$, we can see that the final value of the summation is going to be\n\n$$\ne^{-(t-1-(t-1))w+k_i} = e^{k_{t-i}}\n$$\n\nWell, what about the second last value of the summation then? it's going to be\n\n$$\ne^{-(t-1-(t-2))w+k_{t-2}} = e^{-w + k_{t-i}}\n$$\n\nThis means that we can therefore link $\\alpha_{t-1}$ with $\\alpha_{t}$ by doing\n\n$$\n\\sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i}v_{i} = \\sum_{i=1}^{t-2}e^{-(t-1-i)w+k_i}v_{i} + e^{k_{t-1}}\n$$\n\nWe can perform a similar substituition to get $\\beta_{t} =\\beta_{t-1}+ e^{k_{t-1}}$.\n\n```python\nnum = exp(-exp(decay)) * last_num + exp(k) * v\nden = exp(-exp(decay)) * last_den + exp(k)\n```\n\nWe can then rewrite our equation for $wkv_{t}$ as\n\n$$\nwkv_{t} = \\frac{\\alpha_{t-1} + e^{\\text{bonus} + k_{t}}v_t }{\\beta_{t-1} + e^{\\text{bonus} + k_{t}}}\n$$\n\nwhich corresponds to our implementation of\n\n```py\nwkv = (last_num + exp(bonus + k) * v) /      \\\n          (last_den + exp(bonus + k))\n    rwkv = sigmoid(r) * wkv\n```\n\n### Channel Mixing\n\nThe channel mixing is the short term component of the system - it only has access to the previous value and it takes a weighted sum of the previous and current value that it has calculated.\n\n![](./images/Channel_Mixing.png)\n\nIt's significantly easier to understand. **Note that all these parameters are learned parameters with the exception of the $x_{t-1}$ values**.\n\nOur channel mixing function is implemented as\n\n```python\ndef channel_mixing(x, last_x, mix_k, mix_r, Wk, Wr, Wv):\n    k = Wk @ ( x * mix_k + last_x * (1 - mix_k) )\n    r = Wr @ ( x * mix_r + last_x * (1 - mix_r) )\n    vk = Wv @ np.maximum(k, 0)**2\n    return sigmoid(r) * vk, x\n```\n\n- It's interesting here to note that\n  - $w_k$ has a dimensionality of $[4096,1024]$\n  - $w_v$ has a dimensionality of $[1024,4096]$\n\nThis means that we perform a scaling in the channel_mixing step from an initial dimensionality of 1024 to a dimensionality of 4096. This is very similar to the feed forward network that is used as a transformer.\n\n## Bringing it together\n\nIf we look at the code for RWKV in 100 lines, we notice that the RWKV architecture for the 400M model comprises 24 layers of the same block.\n\nEach block comprises\n\n- 1 initial layer norm ( Constant work since the size of the input is fixed as a (1024,) matrix )\n- 1 time_mixing step\n- 1 layer norm ( Also fixed size input of (1024,) )\n- 1 channel mixing step\n\nAll of the operations that we perform in the time_mixing and channel_mixing step are all going to linearly scale with the size of our context.\n\nThe way I understand it is that since information on all previous token is compressed into a single token value\n\n- Time Mixing requires a constant number of operations per prediction step\n- Channel Mixing also requires a constant number of operations per prediction step\n\nTherefore, for every additional token we want to predict or want to add into our context, we have a constant amount of work to be done for each of these tokens. Therefore, this means that the amount of computational work we need to perform will scale roughly linearly with the amount of tokens we eventually need to predict/proccess.\n",
  "slug": "a-guide-to-rkwv"
}