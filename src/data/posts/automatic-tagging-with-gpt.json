{
  "title": "Classifying Google Map locations with LLMs",
  "date": "2023-07-01T00:00:00.000Z",
  "description": "Using LLMs to automatically tag and categorize your favourite eating spots",
  "categories": [
    "LLMs",
    "Instructor"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n# Introduction\n\n> As usual, you can find the code for this specific article\n> [here](https://github.com/ivanleomk/scrape-google-maps)\n\nIf you've ever used Google Maps, you've definitely struggled to decide where to go to eat. The UI ... frankly sucks beyond belief for an application that has all the data and compute that it has.\n\n![](./images/Google_Maps.png)\n\nThere's not even a simple way to filter the saved places by keywords, categories or even just the location. You need to manually pan and find something that you like.\n\nSo i thought I'd crawl it for the fun of it since I wanted more control over my data. I'd seen some jupyter notebooks but no fine-grained script online so I built my own.\n\n## The Idea\n\nWhen you navigate to a saved location, you see the following\n\n![](./images/Saved_Location.png)\n\nYou get\n\n1. The name\n2. The overall rating\n3. A category that google has assigned to it\n\namong a few other bits of data. But if you use some of the default categories, the amount of data provided is inconsistent. Therefore, here's how our script gets around inconsistent data.\n\nWe simply visit the page that's linked to the listing!\n\n<img src=\"/images/Linked_page.png\" />\n\nWhat makes it even better is that the page link itself contains a lot of valuable information\n\nFor instance, take the following link to FiftyFive Coffe Bar,\n\n```bash\nhttps://www.google.com/maps/place/FiftyFive+Coffee+Bar/@1.3731958,103.8075006,12z/data=!4m11!1m3!11m2!2s9wJ7F7C-bjteOlxRQli8-lZz7jeYIw!3e2!3m6!1s0x31da192386bddc8d:0xed0fef7eae8bf927!8m2!3d1.2795647!4d103.8425105!15sCgEqkgEEY2FmZeABAA!16s%2Fg%2F11k50kg36w?entry=ttu\n```\n\nWe can actually extract out things like the lattitude and longtitude from the link. This helps us to perform useful tasks such as geocoding and reverse geocoding.\n\nEven the HTML isn't too bad for scrapping with unique class names that we can target as seen below\n\n![](./images/Google_Html.png)\n\n## The Code\n\n### Data Models\n\nBefore we start coding out any code, let's start by defining some models\n\n```py\nclass Status(Enum):\n    CLOSED = \"Closed\"\n    TEMPORARILY_CLOSED = \"Temporarily Closed\"\n    OPEN = \"Open\"\n\nclass Listing(BaseModel):\n    title: str\n    rating: Optional[float]\n    number_of_reviews: Optional[int]\n    address: Optional[str]\n    review: str\n    hasDelivery: bool\n    hasTakeaway: bool\n    hasDineIn: bool\n    category: Optional[str]\n    status: str\n    url: str\n    address: str\n    long: float\n    lat: float\n```\n\nThe `Status` class is an enumeration that represents the status of a listing. It has three possible values: `CLOSED`, `TEMPORARILY_CLOSED`, and `OPEN`.\n\nThe `Listing` class is a Pydantic model that defines the structure and validation rules for the data related to a specific listing. The fields in this model include:\n\n- `title`: The title of the listing (a string)\n- `rating`: The rating of the listing (a float, optional)\n- `number_of_reviews`: The number of reviews for the listing (an integer, optional)\n- `address`: The address of the listing (a string, optional)\n- `review`: A review for the listing (a string)\n- `hasDelivery`: A boolean indicating if the listing offers delivery\n- `hasTakeaway`: A boolean indicating if the listing offers takeaway\n- `hasDineIn`: A boolean indicating if the listing offers dine-in\n- `category`: The category of the listing (a string, optional)\n- `status`: The status of the listing, which should be one of the `Status` enumeration values (a string)\n- `url`: The URL of the listing (a string)\n- `long`: The longitude of the listing (a float)\n- `lat`: The latitude of the listing (a float)\n\nNow that we have our models, let's start by writing some simple code\n\n### Setting up Selenium\n\n> I strongly suggest using a virtual environment to follow along\n\nWe can set up a selenium instance to crawl in python by using\n\n```py\nfrom selenium.webdriver.chrome.service import Service\n\nservice = Service()\nservice.start()\n\ndriver = webdriver.Chrome()\nlist_url = // Your list url\ndriver.get(list_url)\ntime.sleep(5)\n\n```\n\n### Crawling the Data\n\nThis simply code chunk is enough to navigate to your list url. All our crawler does thereafter is just\n\n1. Click on each item sequentially\n2. Navigate to the page that links to the item\n3. Extract the data from the page\n4. Go back to the original list url\n\n![](./images/crawling.gif)\n\nSo, before we can even start crawling, it's important to understand how many items we need to crawl. We can do so with a simple while loop\n\n```py\ndiv_element = driver.find_element(\n    By.CSS_SELECTOR,\n    'div[class*=\"m6QErb\"][class*=\"DxyBCb\"][class*=\"kA9KIf\"][class*=\"dS8AEf\"]',\n)\n\nconsole.log(f\"Starting to crawl list page : {list_url}\")\n# Scroll down to the specific div element\n\nlast_height = driver.execute_script(\"return arguments[0].scrollHeight\", div_element)\n\nwhile True:\n    driver.execute_script(\n        \"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", div_element\n    )\n    time.sleep(2)\n\n    html_source = div_element.get_attribute(\"innerHTML\")\n    curr_height = driver.execute_script(\"return arguments[0].scrollHeight\", div_element)\n    if curr_height == last_height:\n        break\n    last_height = curr_height\n```\n\nThis simply checks the height of the div which contains all our saved items and sees if it has increased in size each time.\n\nOnce we navigate to a specific window, we can then extract all the data that we need.\n\nWe first try to find the parent element that contains the data on the listing page\n\n```python\ndef extract_details_from_window(driver, review: str) -> Listing:\n    try:\n        for _ in range(3):\n            try:\n                driver.find_element(\n                    By.CSS_SELECTOR, 'div[class*=\"m6QErb WNBkOb\"]:not(:empty)'\n                )\n            except Exception as e:\n                console.log(\n                    \"Unable to find parent element. Retrying again in 4 seconds...\"\n                )\n                time.sleep(4)\n```\n\nOnce we've validated that we've found the parent element, we parse the content using Beautiful Soup 4.\n\n```python\nparent_container = driver.find_element(\n    By.CSS_SELECTOR, 'div[class*=\"m6QErb WNBkOb\"]:not(:empty)'\n)\n\nsoup = BeautifulSoup(parent_container.get_attribute(\"innerHTML\"), \"html.parser\")\nis_empty = len(soup.contents) == 0\n\nif is_empty:\n    console.log(\"Parent container is empty\")\n    raise ValueError(\"Parent container is empty\")\n```\n\nIn the event that we cannot find any content - this sometimes happens if we just cannot click the item succesfully. We simply raise an error and move on to the next item.\n\nOnce we've extracted the data, we can then extract the data from the page.\n\nHere's an example of the logs for the data data that we extracted from the bearded bella page\n\n```bash\n[17:37:18] Unable to find parent element. Retrying again in 4  main.py:70\n           seconds...\n           Extracted review as Itâ€™s very good - the cold      main.py:100\n           pasta is to die for and the coffee is solid. Solid\n           4/5 would go back\n[17:37:24] Extracted out html from parent container           crawl.py:37\n           Extracted title as Bearded Bella                   crawl.py:39\n           Extracted status as Open                           crawl.py:53\n           Extracted rating as 4.2                            crawl.py:66\n           Extracted rating as 790                            crawl.py:67\n           Extracted address as 8 Craig Rd, Singapore 089668  crawl.py:75\n           Extracted lat as 1.2777283 and long as 103.8428438 crawl.py:82\n           Extracted category as Restaurant                   crawl.py:96\n           Extracted hasDelivery as False                     crawl.py:97\n           Extracted hasTakeaway as True                      crawl.py:98\n           Extracted hasDineIn as True\n```\n\n### Saving the Data\n\nOnce we've extracted out the individual items, we can then write it to a csv file with\n\n```python\ndata = [i for i in data if i is not None]\ncsv_file = \"items.csv\"\nwith open(csv_file, \"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n    writer = csv.writer(file)\n\n    # Write the header row\n    writer.writerow(Listing.__fields__.keys())\n\n    # Write the data rows\n    for item in data:\n        try:\n            writer.writerow(item.dict().values())\n        except Exception as e:\n            print(f\"An error occurred while extracting data: {str(e)}\")\n            import pdb\n\n            pdb.set_trace()\n\n```\n\n> Note that running this specific file will take you quite some time. I suggest running it in the background while you do other things since we implement a good amount of timeouts so we don't get rate limited.\n\n## Classification with GPT\n\nNow that we have our data, we can go beyond just scraping the data and actually do something with it. In my case, I scrapped a total of ~86 different entries so I went ahead and manually rated them on a scale of 1-10.\n\nThis gives us a df with the following columns\n\n```python\n> df.columns()\nIndex(['title', 'rating', 'number_of_reviews', 'address', 'review',\n       'hasDelivery', 'hasTakeaway', 'hasDineIn', 'category', 'status', 'url',\n       'long', 'lat', 'country'],\n      dtype='object')\n```\n\n> Most of the code here for classification was modified from the following tweet which you can check out [here](https://twitter.com/_ScottCondron/status/1674420258080452610)\n\n### Pydantic Models\n\nAs always, we start by defining a simple `pydantic` model to store our data for each individual restaurant\n\n```python\nclass Location(BaseModel):\n    title:str\n    rating:float\n    number_of_reviews:int\n    user_review:str\n    categories:list[str]\n```\n\nWe can then create a simple function to extract all of the data from our dataframe\n\n```python\n\nlocations = []\n\nfor row in df.itertuples():\n    location = Location(\n        title = row.title,\n        rating = row.rating/2,\n        number_of_reviews = row.number_of_reviews,\n        user_review=row.review,\n        categories = [row.category]\n    )\n    locations.append(location)\n\n```\n\n### Yake-ing out the keywords\n\nWe first use use the Yake keyword extractor to extract out all the keywords that are present in our text\n\n> Yake is a light weight unsupervised automatic keyword extraction algorithm that uses a small set of heuristics to capture keywords. You can check it out [here](https://github.com/LIAAD/yake)\n\nWe can install the library by running the following command\n\n```bash\npip install git+https://github.com/LIAAD/yake\n```\n\nWe can then use the following code to extract out the keywords\n\n```py\nimport yake\n\nkw_extractor = yake.KeywordExtractor()\nkeywords = set([])\n\nfor row in df.itertuples():\nformatted_string = f\"title: {row.title}, review: {row.review}, category: {row.category}\"\nnew_keywords = kw_extractor.extract_keywords(formatted_string)\nextracted_keywords = [x[0] for x in new_keywords]\nfor extracted_keyword in extracted_keywords:\nkeywords.add(extracted_keyword)\n```\n\nSince Yake is simply extracting out semantic bits which might have useful information, we end up with certain tags which aren't very useful. Here are some examples that don't really make sense\n\n```python\n['AMAZING','AMAZING DUCK','AMAZING DUCK BREST']\n```\n\n### GPT Keywords\n\nThis is where GPT comes in. We basically get it to look at all our keywords and then generate ~30 categories that can convery the same meaning. If you look at my dataset in the github repo, you'll notice that there are also a good amount of non-english words.\n\nI was a bit lazy to filter it so I decided to just tell GPT to only consider english categories.\n\n```py\npython class Tags(BaseModel):\n    categories: List[str]\n\ndef generate_categories(keywords): keywords_with_new_lines = '\\n'.join(keywords)\n\nprompt = f\"\"\"\n    Invent categories for some restaurants. You are about to be provided with a brief description of a restrautn from google maps.\n\n    Here are some categories that we have. Only consider english categories.\n    {keywords_with_new_lines}\n\n    Create 30 short categories that semantically group these keywords.\n\n    Think step by step\n    \"\"\"\n\n    response = openai.ChatCompletion.create(\n        model = \"gpt-3.5-turbo-16k\",\n        messages = [\n            {'role':'user','content':prompt}\n    ]\n    functions = [\n        {\n            'name': 'output_categories',\n            'description': 'The final list of categories',\n            'parameters':Tags.schema()\n        }\n    ],\n    function_call={\n        'name':'output_categories'\n    }\n)\n\nparsed_json = response.choices[0][\"message\"][\"function_call\"][\"arguments\"]\ncategories = json.loads(parsed_json)[\"categories\"]\nreturn categories\n\nres = generate_categories(list(keywords))\n\n```\n\nSince most of my user reviews had been rather sporadic and inconsistent in length, I thought it wouldn't be useful to force gpt to generate a lot of different recomendations but instead simply focus on a small set of categories - 30 seemed like a good number.\n\n### Categorising\n\nOnce we had our categories, we now need to categorise and assign categories to each individual item with .. gpt again.\n\n```py\n@retry(tries=3, delay=2)\ndef location:Location,categories:list[str]):\n    joined_categories = '\\n'.join(categories)\n    prompt = f\"\"\"\n        Given a Restaurant title and a candid user review, return a new list of 4 categories for the following restaurant\n\n        You can use the following categories\n        {joined_categories}\n\n        Restaurant Title: {location.title},\n        Existing Categories: [{','.join(location.categories)}]\n        User Review: {location.user_review}\n\n        You MUST only response with each chose category separated by a new line.\n        You MUST not say anything after finishing.\n        Your response will be used to tag the paper so don't say anything!\n\n        The 4 Categories:\n    \"\"\n\n    response = openai.ChatCompletion.create(\n        model = \"gpt-3.5-turbo-16k\",\n        messages = [\n            {'role':'user','content':prompt}\n        ]\n    )\n    return response[\"choices\"][0][\"message\"][\"content\"].split(\"\\n\")\n\n```\n\nOver here, we're using the `retry` package which allows for automatic retries in case the API fails or the function throws an error. This is useful since the API can sometimes fail due to rate limits.\n\nI'd like to also throw an error if a response takes too long but I haven't figured out how to do that yet.\n\n### Putting it all together\n\nOnce we've got the categorisation functionality down, all we need is to then work on the actual classification. We can do this by simply iterating through each restaurant and then tagging it with the categories that we generated earlier with our `tag_restaurant` function.\n\nWe can do so with the following loop.\n\n```py\nparsed_locations = []\n\nfor location in locations:\n    new_categories = tag_restaurant(location,categories)\n    new_location = location.copy()\n    new_location.categories.extend(new_categories)\n\n    unique_categories = list(\n        set(\n            [i.lower().strip() for i in new_location.categories]\n        )\n    )\n\n    new_location.categories = [i.title() for i in unique_categories]\n\n    parsed_locations.append(new_location)\n\n```\n\nbefore writing it to a csv with the following function\n\n```py\ndef write_locations_to_csv(locations: List[Location], file_name: str):\n    fieldnames = list(Location.schema()[\"properties\"].keys())\n\n    with open(file_name, \"w\", newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for location in locations:\n            writer.writerow(location.dict())\n\nwrite_locations_to_csv(parsed_locations, \"locations.csv\")\n```\n\nThe results aren't honestly too bad - we can see from the quick screengrab that I took that it was able to accurately tag certain places correctly - the function sometime generated duplicate tags and so the use of a set in our code was useful to remove duplicates.\n\n## Building a UI\n\nI used streamlit to build a quick UI for us to iterate through the different categories and see what the results were like.\n\n```python\nimport streamlit as st\nimport pandas as pd\nfrom pydantic import BaseModel\nfrom typing import List\n\ndf = pd.read_csv(\"./locations.csv\")\n\nparsed_categories = [[j.strip() for j in i[1:-1].replace(\"'\", \"\").split(\",\")] for i in df[\"categories\"].to_list()]\nfor cat_list in parsed_categories:\n    for cat in cat_list:\n        category_set.add(cat)\n\nst.title(\"Location Filter\")\n\n# Filter by title\n\ntitle_filter = st.sidebar.text_input(\"Search by title\")\nfiltered_df = df[df[\"title\"].str.contains(title_filter, case=False)]\n\n# Filter by categories\n\nunique_categories = list(category_set)\nselected_categories = st.sidebar.multiselect(\"Filter by categories\", unique_categories)\nfiltered_df = filtered_df[filtered_df[\"categories\"].apply(lambda x: any(category in x for category in selected_categories) or len(selected_categories) == 0)]\n\nprint(unique_categories)\n\n# Filter by number of reviews\n\nmin_reviews, max_reviews = st.sidebar.slider(\n\"Filter by number of reviews\",\nint(df[\"number_of_reviews\"].min()),\nint(df[\"number_of_reviews\"].max()),\n(0, int(df[\"number_of_reviews\"].max())),\n)\nfiltered_df = filtered_df[\n(filtered_df[\"number_of_reviews\"] >= min_reviews)\n& (filtered_df[\"number_of_reviews\"] <= max_reviews)\n]\n\n# view_df = filtered_df[[\"title\", \"number_of_reviews\", \"categories\"]]\n\n# Display the filtered DataFrame\n\nst.write(filtered_df[[\"title\", \"number_of_reviews\", \"rating\", \"user_review\"]])\n\n```\n\nThis is what the UI looks like\n\n![](./images/streamlit-ui.gif)\n",
  "slug": "automatic-tagging-with-gpt"
}