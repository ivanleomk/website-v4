{
  "title": "Are your eval improvements just pure chance?",
  "date": "2024-12-04T00:00:00.000Z",
  "description": "Grokking simple statistical analysis for LLM evals",
  "categories": [
    "LLMs",
    "Evals"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n# Are your eval improvements just pure chance?\n\nA step that's often missed when benchmarking retrieval methods is determining if any performance difference is due to random chance. Without this crucial step, you might invest in a new system upgrade that's outperformed your old one by pure chance.\n\nIf you're comparing retrieval methods, you'll often want to know if the improvements you're seeing are due to random chance.\n\nIn this article, we'll use a simple case study to demonstrate how to answer this question, introducing a new library called `indomee` (a playful nod to both \"in-domain\" evaluation and the beloved instant noodle brand in Southeast Asia) that makes this analysis significantly easier.\n\nWe'll do so in three steps:\n\n1. First we'll simulate some fake data using `numpy`\n2. Then we'll demonstrate how to do bootstrapping using nothing but `numpy` before visualising the results with `matplotlib`\n3. Finally we'll perform a paired t-test to determine if the differences are statistically significant\n\nLet's start by installing the necessary libraries:\n\n```bash\npip install indomee numpy pandas scipy matplotlib\n```\n\n## Why RAG metrics matter\n\nBefore diving into the statistical analysis, let's understand why these metrics matter for Retrieval Augmented Generation (RAG):\n\n### Looking to the Future\n\nAs LLMs become more capable, two key trends are emerging:\n\n- Improved context understanding: Newer models are getting better at identifying and utilizing relevant information from their context window\n- Longer context windows: Gemini has a context window that can take 2 million tokens - that's the entire harry potter series multiplied by 10 times over and a bit more.\n\nThis means that:\n\n1. Having high-quality retrieval becomes even more critical - better models can make better use of good context\n2. We can afford to retrieve more documents since context windows are expanding\n3. The cost of retrieving irrelevant documents is decreasing (as models get better at ignoring noise)\n\nThis is why measuring both recall and position (through MRR) is crucial:\n\n- **Recall@k**: Measures whether relevant documents appear in the top k retrieved results. For RAG, high recall is crucial because even if relevant information is mixed with some noise, newer models are increasingly good at finding and using it. A recall@5 of 0.8 means that 80% of the time, the relevant document appears in the top 5 results.\n\n- **Mean Reciprocal Rank (MRR)**: Focuses on where the first relevant document appears in the results. Even with longer context windows, having relevant information appear earlier helps ensure it won't get truncated and gives models the best chance of using it effectively.\n\n### Business-Driven Evaluation\n\nThe specific k values you choose for evaluation should be driven by your business needs:\n\n- Search engines might care about recall@25 or higher since users can scroll through results\n- Recommendation Systems might prioritize recall@5 since they need to be concise\n- Research assistants might use recall@50+ to gather extensive information\n\nThis is why being able to calculate metrics at different k values is crucial - it lets you align your evaluation with your specific use case. Most importantly, it allows you to see whether how retrieval performs at different k values.\n\n## Comparing Three Retrieval Methods\n\nLet's say you're comparing three retrieval methods based off recall@10. This is a metric between 0 and 1 that measures how many relevant items are retrieved in the top 10 results.\n\nYou have the following three methods:\n\n- Baseline: Your current approach\n- Method 1: A promising new approach\n- Method 2: Another alternative\n\nSince we don't have real data, we'll simulate some fake data using `numpy`. We'll use a normal distribution to generate random scores and then ensure they're between 0 and 1 using `clip(0,1)`:\n\n```python\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n# Simulate data for 200 test queries\nn_tests = 200\nbaseline = np.random.normal(0.4, 0.08, n_tests).clip(0, 1)\nmethod_1 = np.random.normal(0.45, 0.1, n_tests).clip(0, 1)\nmethod_2 = np.random.normal(0.49, 0.14, n_tests).clip(0, 1)\n\ndf = pd.DataFrame({\n    \"test_id\": range(1, n_tests + 1),\n    \"baseline\": baseline,\n    \"method_1\": method_1,\n    \"method_2\": method_2\n})\n\n# Initial data simulation output\nprint(df.mean().round(2))\n\ntest_id     100.50\nbaseline      0.40\nmethod_1      0.46\nmethod_2      0.48\ndtype: float64\n\n# Shows the mean values for each method - baseline performs worst, method_2 performs best\n```\n\nLooking at the means:\n\n- Method 2 is 20% better than baseline\n- Method 2 is 4% better than Method 1\n\nBut is Method 2 really better, or is this just random chance?\n\nWe need two things to answer this:\n\n1. Bootstrapping to estimate confidence intervals\n2. Statistical tests to validate the differences\n\n## Measuring Uncertainty with Bootstrapping\n\nBootstrapping helps us understand how much our results might change if we ran the experiment again. It works by creating new datasets from our original dataset by randomly selecting samples with replacement. Sampling with replacement means that an item in our original dataset can be selected more than once when we're selecting samples.\n\nBy repeating this multiple times, we can create a large diversity of potential outcomes. In the code snippet below, we repeat this 1000 times, allowing us to see how stable or uncertain our results are:\n\n```python\nimport numpy as np\n\nSAMPLE_SIZE = 200\nNUM_SAMPLES = 1000\n\n# Lists to store bootstrapped means\nbaseline_means = []\nmethod_1_means = []\nmethod_2_means = []\n\n# Perform bootstrapping\nfor _ in range(NUM_SAMPLES):\n    sample = df.sample(SAMPLE_SIZE, replace=True)\n    baseline_means.append(sample[\"baseline\"].mean())\n    method_1_means.append(sample[\"method_1\"].mean())\n    method_2_means.append(sample[\"method_2\"].mean())\n\n# Compute mean of bootstrapped samples\nmethod_names = ['Baseline', 'Method 1', 'Method 2']\nall_means = [baseline_means, method_1_means, method_2_means]\nmean_estimates = [np.mean(means) for means in all_means]\n\n# Calculate confidence intervals\nci_lower = [np.percentile(means, 2.5) for means in all_means]\nci_upper = [np.percentile(means, 97.5) for means in all_means]\nerror = [[m - l for m, l in zip(mean_estimates, ci_lower)],\n         [u - m for m, u in zip(mean_estimates, ci_upper)]]\n\n# Plotting code\nplt.figure(figsize=(8, 6))\nplt.bar(range(len(method_names)), mean_estimates, yerr=error, align='center',\n        alpha=0.7, capsize=10, color='skyblue')\nplt.xticks(range(len(method_names)), method_names)\nplt.ylabel('Mean Value')\nplt.title('Bootstrap Confidence Intervals')\nplt.grid(True, axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n```\n\n### Simplifying Bootstrap Analysis with `indomee`\n\nNow that we've seen how to perform bootstrapping manually, let's see how indomee simplifies this process.\n\nPreviously, you'd need all that bootstrapping code. With indomee, it's just:\n\n```python\nfrom indomee import bootstrap_from_results\n\nbaseline_stats = bootstrap_from_results(df[\"baseline\"].tolist())\nmethod_1_stats = bootstrap_from_results(df[\"method_1\"].tolist())\nmethod_2_stats = bootstrap_from_results(df[\"method_2\"].tolist())\n\n# Bootstrap metric output\nprint(baseline_stats)\n\nBootstrapMetric(\n    name='bootstrap_metric',  # Identifier for the metric\n    value=0.47800829518587357,  # Mean value across all bootstrap samples\n    ci_lower=0.4582167411327398,  # Lower bound of 95% confidence interval\n    ci_upper=0.498828905051887  # Upper bound of 95% confidence interval\n)\n```\n\n### Computing metrics at different k values\n\n`indomee` makes it easy to calculate recall and mrr at different k values:\n\n```python\nfrom indomee import calculate_mrr, calculate_recall\n\n# Basic metric calculation\nmrr = calculate_mrr([\"apple\", \"banana\", \"orange\"], [\"banana\", \"orange\", \"grape\"])\nprint(\"MRR:\", mrr)\n# > MRR: 0.5\n\nrecall = calculate_recall([\"apple\", \"banana\", \"orange\"], [\"banana\"])\nprint(\"Recall:\", recall)\n# > Recall: 1\n\n# Metrics at different k values\nmetrics = calculate_metrics_at_k(\n    metrics=[\"recall\"],\n    preds=[\"cat\", \"dog\", \"fish\", \"bird\", \"hamster\"],\n    labels=[\"fish\"],\n    k=[1, 2, 3, 4, 5],\n)\n\nfor metric in metrics:\n    print(f\"- {metric} -> {metrics[metric]}\")\n\n# Output:\n- recall@1 -> 0.0  # Fish not in first position\n- recall@2 -> 0.0  # Fish not in first two positions\n- recall@3 -> 1.0  # Fish found at position 3\n- recall@4 -> 1.0  # Fish still found when looking at top 4\n- recall@5 -> 1.0  # Fish still found when looking at top 5\n```\n\n### Working with Raw Bootstrap Data\n\nWhat makes indomee really powerful is that we can bootstrap not with the mean of the results but the raw data itself. This allows us to:\n\n1. Do a single bootstrap with `bootstrap_sample`\n2. Do multiple bootstraps with `bootstrap`\n\n```python\n# Single bootstrap sample\nresult = bootstrap_sample(\n    preds=[[\"a\", \"b\"], [\"c\", \"d\"], [\"e\", \"f\"]],\n    labels=[[\"a\", \"b\"], [\"c\", \"d\"], [\"e\", \"f\"]],\n    sample_size=10,\n    metrics=[\"recall\"],\n    k=[1, 2, 3],\n)\n\n# Multiple bootstraps\nresult = bootstrap(\n    preds=[[\"a\", \"b\"], [\"c\", \"d\"], [\"e\", \"f\"]],\n    labels=[[\"a\", \"b\"], [\"c\", \"d\"], [\"e\", \"f\"]],\n    n_samples=2,\n    sample_size=10,\n    metrics=[\"recall\"],\n    k=[1, 2, 3],\n)\n```\n\nThe result gives us access to:\n\n1. The raw samples used in each bootstrap iteration\n2. The metrics calculated for each iteration\n3. Summary statistics across all iterations\n\nThis makes it easy to do more advanced analysis depending on your specific use case.\n\n## Testing for Statistical Significance\n\nThe t-test is a statistical method that helps us determine if the differences we observe between two methods are statistically significant or just due to random chance.\n\nWhen we look at our bootstrap plots, we can see some overlap in the confidence intervals between methods. While this gives us a visual indication, the t-test provides a formal statistical framework to quantify this difference:\n\n1. The t-test calculates a t-statistic and p-value. The t-statistic measures how many standard deviations the difference between means is from zero while the p-value tells us the probability of observing such a difference if there was actually no real difference between the methods.\n\n2. We use a paired t-test here because our measurements are related (same queries used for each method) and each data point in one method has a natural pairing with a data point in the other method. This helps control for query-specific variations.\n\nFirst, let's see the manual implementation:\n\n```python\nfrom scipy.stats import ttest_rel\nimport pandas as pd\n\n# Calculate the mean for each method\nmethod_1 = df[\"method_1\"].tolist()\nmethod_2 = df[\"method_2\"].tolist()\nbaseline = df[\"baseline\"].tolist()\n\nt_stat_baseline_method1, p_val_baseline_method1 = ttest_rel(baseline, method_1)\nt_stat_baseline_method2, p_val_baseline_method2 = ttest_rel(baseline, method_2)\nt_stat_method1_method2, p_val_method1_method2 = ttest_rel(method_1, method_2)\n\n# Create a DataFrame with the t-test results\nresults_df = pd.DataFrame(\n    {\n        \"Comparison\": [\"Baseline vs M1\", \"Baseline vs M2\", \"M1 vs M2\"],\n        \"T statistic\": [\n            t_stat_baseline_method1,\n            t_stat_baseline_method2,\n            t_stat_method1_method2,\n        ],\n        \"P Value\": [\n            p_val_baseline_method1,\n            p_val_baseline_method2,\n            p_val_method1_method2,\n        ],\n    }\n)\n\n# Format the numeric columns\nresults_df[\"T statistic\"] = results_df[\"T statistic\"].round(2)\nresults_df[\"P Value\"] = results_df[\"P Value\"].map(\"{:.2e}\".format)\n```\n\n### Simplifying t-tests with `indomee`\n\nWith indomee, running t-tests is straightforward. Better yet, we automatically do a pairwise comparison for all of the methods you pass in with the option for custom labels for each method.\n\n```python\nfrom indomee import perform_t_tests\n\nresults = perform_t_tests(\n    baseline, method_1, method_2,\n    names=[\"Baseline\", \"Method 1\", \"Method 2\"],\n    paired=True\n)\nprint(results)\n\n    Comparison  T statistic  P Value\nBaseline vs M1        -7.42 3.30e-12  # Highly significant difference\nBaseline vs M2        -6.91 6.55e-11  # Highly significant difference\n      M1 vs M2        -1.59 1.14e-01  # Not significant (p > 0.05)\n```\n\nThis produces a clean DataFrame showing:\n\n- The methods being compared\n- T-statistics and p-values\n- Whether differences are significant at p < 0.05\n\n## Conclusion\n\nAs LLMs continue to evolve with better context understanding and longer context windows, having robust evaluation of retrieval systems becomes even more crucial. Statistical analysis helps ensure that improvements are real and not just random chance.\n\nWithout being able to perform statistical analysis, you might invest in a new system upgrade that's outperformed your old one by pure chance. That's a huge time sink and a waste of resources for any company looking to move fast and build a great product.\n\n`indomee` makes it easy to implement statistical analysis for your retrieval pipeline. Give it a try today with `pip install indomee`.\n",
  "slug": "report-error-bars"
}