{
  "title": "Building Reliable LLM Applications",
  "date": "2025-01-27T00:00:00.000Z",
  "description": "How to build reliable LLM applications with structured outputs, synthetic data and binary metrics",
  "categories": [
    "Evals",
    "Instructor",
    "Synthetic Data"
  ],
  "authors": [
    "ivanleomk"
  ],
  "content": "\n# Why Structured Outputs matter for LLM Applications in 2025\n\n> I gave a short talk at NUS in January 2025 about structured outputs and how they enable faster iteration and testing when building language models. I've written up a more detailed version of the talk here as well as provided the slides below.\n\nLLM applications in 2025 face a unique challenge: while they enable rapid deployment compared to traditional ML systems, they also introduce new risks around reliability and safety.\n\nIn this article, I'll explain why structured outputs remain crucial for building robust LLM applications, and how they enable faster iteration and testing.\n\n<iframe \n    src=https://r2-workers.ivanleomk9297.workers.dev/presentation.pdf\n    width=\"100%\" \n    height=\"600px\" \n    style=\"border: none;\">\n</iframe>\n\n## The New Development Paradigm\n\nTraditional ML applications follow a linear path: collect data, train models, then deploy. LLM applications flip this on its head:\n\n1. Deploy quickly with base capabilities\n2. Collect real user data during production\n3. Build specialized infrastructure around the model over time\n\nThis accelerated timeline brings both opportunities and challenges. While we can get to market faster, we need robust guardrails to ensure reliability and safety.\n\n## Structured Outputs\n\n### Why Raw JSON is not enough\n\nMany developers start by parsing raw JSON from LLM outputs, but this approach is fraught with issues:\n\n- Inconsistent output formats and parsing errors with malformed JSON\n- Difficulty handling edge cases\n- Challenges with retries and validation\n\nFunction calling, supported by most major providers, solves these problems by providing structured, validated outputs. Here's a simple example using the Instructor library:\n\n```python\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_openai(OpenAI())\n\nuser_info = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}]\n)\n```\n\nWith `instructor`, we get validated Pydantic objects, which are easier to work with, more reliable and much more powerful relative to a simple `JSON.parse` method. More importantly, we get the following benefits\n\n1. Type safety and IDE support\n2. Easy integration with existing codebases since we have a defined response type ( and other libraries that support Pydantic )\n3. Consistent interface across different providers using the same `openai` api interface.\n\n### Beyond Simple Validation\n\nStructured Outputs enable sophisticated capabilities like\n\n1. **Class Methods** : Since Pydantic objects are classes, we can define methods on them to make them more powerful.\n2. **Field Repair** : Field validators aren't just for validation, we can use them to repair fields that are the result of mispellings or other errors using deterministic logic\n3. **Downstream Processing** : Since we know the output format, it's easy to process the output in a structured way. This allows us to build more complex workflows that leverage things like streaming.\n\nFor instance, given say a set of known clothing categories, we can use libraries like `fuzzywuzzy` to repair mispellings in the output.\n\n```python\nfrom pydantic import BaseModel, field_validator\nfrom fuzzywuzzy import fuzz\n\nclass Product(BaseModel):\n    category: str\n\n    @field_validator(\"category\")\n    def validate_category(cls, v: str) -> str:\n        known_categories = [\"t-shirts\", \"shirts\", \"pants\"]\n        matches = [(fuzz.ratio(v.lower(), cat), cat) for cat in known_categories]\n        best_match = max(matches, key=lambda x: x[0])\n        if best_match[0] > 80:  # Threshold for fuzzy matching\n            return best_match[1]\n        raise ValueError(f\"No matching category found for {v}\")\n```\n\nBy matching these user inputs to known taxonomies and handling spelling variations and typos, we can improve the quality of our data and make it more useful for downstream tasks. Other useful examples include defining the class methods to execute database queries once we've extracted the relevant parameters using a tool call as seen below from an [article I wrote previously on going beyond vector search with RAG](https://www.timescale.com/blog/rag-is-more-than-just-vector-search).\n\n```python\nclass SearchIssues(BaseModel):\n    \"\"\"\n    Use this when the user wants to get original issue information from the database\n    \"\"\"\n\n    query: Optional[str]\n    repo: str = Field(\n        description=\"the repo to search for issues in, should be in the format of 'owner/repo'\"\n    )\n\n    async def execute(self, conn: Connection, limit: int):\n        if self.query:\n            embedding = (\n                OpenAI()\n                .embeddings.create(input=self.query, model=\"text-embedding-3-small\")\n                .data[0]\n                .embedding\n            )\n            args = [self.repo, limit, embedding]\n        else:\n            args = [self.repo, limit]\n            embedding = None\n\n        sql_query = Template(\n            \"\"\"\n            SELECT *\n            FROM {{ table_name }}\n            WHERE repo_name = $1\n            {%- if embedding is not none %}\n            ORDER BY embedding <=> $3\n            {%- endif %}\n            LIMIT $2\n            \"\"\"\n        ).render(table_name=\"github_issues\", embedding=embedding)\n\n        return await conn.fetch(sql_query, *args)\n```\n\nBecause we know the output format, building out workflows that leverage structured outputs is much easier. Here's an example of how we can stream out the result of a structured extraction query in real time as the chunks are streaming in.\n\n![Streamed Results](./images/partial.gif)\n\nThis is a simple example, but it shows how structured outputs enable more complex workflows that are not possible with raw JSON.\n\n## Prioritising Iteration Speed\n\nBinary metrics provide a fast, reliable way to evaluate LLM system performance before investing in expensive evaluation methods. By nailing down the simplest metrics first, we can iterate quickly and make sure we're on the right track before spending our time on more complex methods that might not be as useful.\n\nLet's look at three key areas where simple binary metrics shine:\n\n### RAG Systems\n\nInstead of immediately jumping to complex LLM-based evaluation, start by measuring retrieval quality:\n\n```python\ndef calculate_recall_at_k(queries: List[str], relevant_docs: List[str], k: int) -> float:\n    hits = 0\n    for query, relevant in zip(queries, relevant_docs):\n        retrieved = retrieve_documents(query, k=k)\n        if relevant in retrieved:\n            hits += 1\n    return hits / len(queries)\n```\n\nMost importantly, we can use these queries to test different approaches to retrieval because we can just run the same queries on each of them and see which performs the best. If we were comparing say - BM25, Vector Search and Vector Search with a Re-Ranker step, we can now identify what the impact on recall, mrr and latency is.\n\nSay we generate 100 queries and we find that we get the following results\n\n| Method             | Recall@5 | Recall@10 | Recall@15 |\n| ------------------ | -------- | --------- | --------- |\n| BM25               | 0.72     | 0.78      | 0.82      |\n| Vector Search      | 0.75     | 0.81      | 0.85      |\n| Vector + Re-Ranker | 0.69     | 0.75      | 0.79      |\n\nThis simple metric can reveal important insights:\n\n- If BM25's recall@15 matches vector search's recall@10, you might prefer BM25 for its simplicity\n- If adding a reranker drops recall significantly, you may need to adjust your filtering\n- When comparing embedding models, you can quickly see which performs better for your specific use case\n\n### Text-to-SQL\n\nText-to-SQL is more complex than simple query generation. Consider how experienced data analysts actually work:\n\n1. First, they identify relevant tables in the schema\n2. Then, they search for similar queries others have written\n3. Next, they consult stakeholders to understand what each field means, the hidden relationships between the fields and company-specific calculation methods\n4. Finally, they write the query\n\nThis process reveals why text-to-SQL is fundamentally a RAG problem. While modern LLMs can easily generate simple queries like `SELECT * FROM users`, real-world queries require rich context about company-specific conventions and business logic.\n\nFor example, calculating \"month-over-month growth\" might mean:\n\n- Last 30 days vs previous 30 days in one company\n- Calendar month comparisons in another\n- Fiscal month definitions in yet another\n\nBy ensuring that we're able to obtain the right context from the database, we can now test different approaches for retrieval. This ensures that given some sort of user query, our system always has the necessary context before attempting query generation. This also means that it's easier to debug failures and [progressively improve system performance in a measurable way](https://www.timescale.com/blog/enhancing-text-to-sql-with-synthetic-summaries).\n\n### Tool Selection\n\nTool selection in LLM applications can be evaluated using classic precision-recall metrics, providing clear quantitative insights into system performance.\n\n1. Precision measures how many of our selected tools were actually necessary\n2. Recall tracks whether we selected all the tools we needed.\n\nThis creates an important tension: we could achieve perfect recall by selecting every available tool, but this would tank our precision and likely create operational issues.\n\nThe balance between these metrics becomes crucial when considering real-world constraints. Unnecessary tool calls might incur API costs, add latency, or introduce potential failure points.\n\nFor instance, if a shopping assistant unnecessarily calls a weather API for every product search, it adds cost and complexity without value. By tracking precision and recall over time, we can make data-driven decisions about model selection and prompt engineering, moving beyond gut feelings to measurable improvements in tool selection accuracy.\n\nThis framework helps teams optimize for their specific needs â€“ whether that's maximizing precision to reduce costs, or prioritizing recall to ensure critical tools aren't missed.\n\n## Query Understanding\n\nUnderstanding user behavior at scale requires automated ways to analyze and categorize conversation patterns.\n\n[Kura](https://usekura.xyz) is a library that uses language models to build hierarchical clusters of user queries, helping teams understand broad usage patterns while preserving privacy. Instead of relying on manual annotation or traditional topic modeling, Kura progressively combines similar conversations into meaningful clusters that reflect natural usage patterns.\n\n### The Value of Query Understanding\n\nIt's important to invest in query understanding early because it's vital to make data-driven decisions about product development and resource allocation. When these high level cateogires are combined with metrics like query volume and user feedback, we can\n\n- Identify common user patterns and needs\n- Prioritize feature development based on usage\n- Discover emerging use cases over time\n\nThis helps us to focus on what really moves the needle - Segmentation is the name of the game here.\n\n### How Kura Works\n\nKura takes a hierarchical approach to clustering:\n\n1. Summarizes individual conversations into high-level user requests\n2. Groups similar requests into initial clusters\n3. Progressively combines clusters into broader categories\n4. Generates human-readable descriptions for each level\n\nFor example, what might start as separate clusters for \"React component questions\" and \"Django API issues\" could be combined into a higher-level \"Web Development Support\" category that better reflects the technical nature of these conversations.\n\n### Validating with Synthetic Data\n\nTo validate Kura's effectiveness, we tested it with synthetic data generated across multiple dimensions:\n\n- High-level categories (e.g., technical development, content creation)\n- Specific subcategories (e.g., React development, microservices)\n- Varied conversation styles (different tones, lengths, user goals)\n- Different conversation flows (varying turns and response patterns)\n\nIn tests with 190 synthetic conversations, Kura successfully reconstructed the original high-level categories while discovering meaningful new groupings - for instance, combining API documentation questions with software engineering queries based on their technical similarity.\n\n### Getting Started with Kura\n\nYou can try Kura at usekura.xyz or install it directly via pip using the command `pip install kura`.\n\nThe library offers both a simple CLI interface for quick analysis and a Python API for more complex integrations.\n\nWhether you're looking to understand user behavior or prioritize feature development, Kura provides a systematic way to analyze conversations at scale while maintaining user privacy.\n\n## Conclusion\n\nBuilding reliable LLM applications in 2025 requires a systematic approach focused on measurable outcomes. By combining structured outputs, binary metrics, and sophisticated query understanding, teams can create robust systems that improve steadily over time:\n\n- Structured outputs through function calling and libraries like Instructor provide a foundation for reliable data handling and type safety, enabling sophisticated workflows and easier debugging\n- Simple binary metrics like recall and precision offer quick feedback loops for iterating on core functionality before investing in complex evaluations\n- Tools like Kura help teams understand usage patterns at scale, directing development efforts where they'll have the most impact\n\nMost importantly, this approach transforms gut feelings into quantifiable metrics, allowing teams to make data-driven decisions about system improvements. The goal isn't to build a perfect system immediately, but to create one that can be measured, understood, and systematically enhanced over time.\n\nAs LLM applications continue to evolve, the teams that succeed will be those that combine rapid deployment with robust evaluation frameworks, ensuring their systems remain reliable and effective as they scale.\n\n## Useful Links\n\nHere are the links that I mentioned which are compiled nicely for you to check out.\n\n- [Kura](https://usekura.xyz) : A library for understanding user queries at scale\n- [CLIO](https://www.anthropic.com/research/clio) : How Anthropic uses language models to understand what users are using Claude for\n- [RAG is more than just vector search](https://www.timescale.com/blog/rag-is-more-than-just-vector-search) : Why vector search is not enough for any production RAG system\n- [Enhancing Text-to-SQL with Synthetic Summaries](https://www.timescale.com/blog/enhancing-text-to-sql-with-synthetic-summaries) : How to use synthetic data to improve text-to-SQL systems\n",
  "slug": "building-reliable-llm-applications"
}